{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/kotsos/github/hua-thesis\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kotsos/anaconda3/envs/gendis_test/lib/python3.6/site-packages/deap/tools/_hypervolume/pyhv.py:33: ImportWarning: Falling back to the python version of hypervolume module. Expect this to be very slow.\n",
      "  \"module. Expect this to be very slow.\", ImportWarning)\n",
      "/home/kotsos/anaconda3/envs/gendis_test/lib/python3.6/importlib/_bootstrap.py:219: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
      "  return f(*args, **kwds)\n",
      "/home/kotsos/anaconda3/envs/gendis_test/lib/python3.6/importlib/_bootstrap.py:219: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "print(module_path)\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from statistics import mean\n",
    "from random import choice\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from gendis.genetic import GeneticExtractor\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tools.data_extraction import DataExtractor\n",
    "from tools.trajectory_generator import TrajectoryGenerator\n",
    "from tools.utils import standardize_data, print_genetic_param, print_settings, set_movements, angle_diff, get_distance\n",
    "from tools.experiments import Experiments\n",
    "\n",
    "np.random.seed(1337)  # Random seed for reproducibility\n",
    "\n",
    "tr_gen_options = {\"samples\": 25,\n",
    "                  \"freq\": 3,\n",
    "                  \"reset_data\": True}\n",
    "dt_gen_options = {\"n_test\": 150}\n",
    "\n",
    "train_test_options = {\"split\": 25}\n",
    "\n",
    "df_csv_options = {\"ts_class\": \"Bearing\"}\n",
    "\n",
    "gen_options = {\"population_size\": 20,\n",
    "               \"iterations\": 5,\n",
    "               \"verbose\": True,\n",
    "               \"normed\": True,\n",
    "               \"add_noise_prob\": 0.0,\n",
    "               \"add_shapelet_prob\": 0.3,\n",
    "               \"wait\": 10,\n",
    "               \"plot\": True,\n",
    "               \"remove_shapelet_prob\": 0.3,\n",
    "               \"crossover_prob\": 0.66,\n",
    "               \"n_jobs\": 4}\n",
    "\n",
    "settings = {\"trajectory_generator_options\": tr_gen_options,\n",
    "            \"data_generation_options\": dt_gen_options,\n",
    "            \"train_test_options\":train_test_options,\n",
    "            \"define_csvs_option\": df_csv_options,\n",
    "            \"genetic_options\": gen_options}\n",
    "\n",
    "tr_gen_options = settings[\"trajectory_generator_options\"]\n",
    "dt_gen_options = settings[\"data_generation_options\"]\n",
    "df_csvs_options = settings[\"define_csvs_option\"]\n",
    "train_test_options = settings[\"train_test_options\"]\n",
    "genetic_options = settings[\"genetic_options\"]\n",
    "\n",
    "first_movement = ['creeping_line_left']\n",
    "second_movement = ['random']\n",
    "movements = {'first_movement': first_movement,'second_movement': second_movement}\n",
    "set_movements(movements)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Create directory 'data' \n",
      "now creating data for movement: creeping_line_left\n",
      "now creating data for movement: random\n",
      "Done with generator\n",
      "\n",
      "Reading the data files....Done reading files\n",
      "The train samples length is:6000\n",
      "The test samples length is:1500\n",
      "\n",
      "Creating x_train.csv--y_train.csv and x_test.csv--y_test.csv ...Done with train.csv ...Done with test.csv\n",
      "Loading the csv files to the appropriate train and test arrays(nparrays)...Done\n"
     ]
    }
   ],
   "source": [
    "# Create files if not created\n",
    "tr_gen = TrajectoryGenerator(**tr_gen_options)\n",
    "tr_gen.data_generation(**dt_gen_options)\n",
    "# Read in the datafiles\n",
    "dex = DataExtractor()\n",
    "train_df, test_df = dex.train_test_dataframes(**train_test_options)\n",
    "print(\"The train samples length is:{0}\".format(len(train_df[0] * train_test_options[\"split\"]*2)))\n",
    "print(\"The test samples length is:{0}\\n\".format(len(test_df[0] * train_test_options[\"split\"]*2)))\n",
    "dex.define_csv(**df_csvs_options)\n",
    "\n",
    "x_train, y_train, x_test, y_test = dex.load_datasets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_sampling(x,sampling,data_index):\n",
    "    i = 1\n",
    "    sampling_acc = True\n",
    "    down_limit = (x[1]-sampling)\n",
    "    up_limit = (x[1]+sampling)\n",
    "    while i<=sampling:\n",
    "        if 0 <=down_limit and up_limit < len(data_index):\n",
    "            if (data_index[x[1]-i] == True) or (data_index[x[1]+i] == True):\n",
    "                sampling_acc = False\n",
    "                break\n",
    "        i = i + 1      \n",
    "    return sampling_acc\n",
    "\n",
    "\n",
    "def dist_and_bearing_diff(data):\n",
    "    all_distances = []\n",
    "    bearing_diff = []\n",
    "    data_size=len(data)\n",
    "    i = 0\n",
    "    while i<data_size:\n",
    "        if i + 1 >=data_size:\n",
    "            break\n",
    "        bearing_1, bearing_2 = data[\"HEADING\"].iloc[i], data[\"HEADING\"].iloc[i+1] \n",
    "        bearing_diff.append([abs(bearing_2 - bearing_1),i])\n",
    "        lat_1, lon_1, lat_2, lon_2 = data[\"LAT\"].iloc[i], data[\"LON\"].iloc[i], data[\"LAT\"].iloc[i+1], data[\"LON\"].iloc[i+1]\n",
    "        all_distances.append(get_distance(lat_1,lon_1,lat_2,lon_2))\n",
    "        i = i +1\n",
    "    return bearing_diff, all_distances \n",
    "\n",
    "\n",
    "def fitting_indexes(arr,new_size):\n",
    "    i = 0\n",
    "    r_arr = []\n",
    "    while i <len(arr):\n",
    "        if i+1 >=len(arr):\n",
    "            break\n",
    "        r_arr.append(arr[i+1]-arr[i])\n",
    "        i = i + 1\n",
    "        \n",
    "    mean_space = mean(r_arr)\n",
    "    i = 0 \n",
    "    while i<len(arr)<new_size:\n",
    "        if i+1 >=len(arr):\n",
    "            break\n",
    "        diff = arr[i+1]-arr[i]\n",
    "        if mean_space < diff:\n",
    "            step = int(diff/2)\n",
    "            new_index = arr[i] + step\n",
    "            arr.insert(i+1,new_index)\n",
    "            i = i + 1\n",
    "        i = i + 1\n",
    "    return arr\n",
    "\n",
    "def scalling_down_windowed(data,n_sample,turn_sensitivity=35):\n",
    "    if len(data) <= n_sample :\n",
    "        return data\n",
    "    size_correction = int(len(data) / n_sample) * n_sample\n",
    "    data=data[:size_correction]\n",
    "    data_size=len(data)\n",
    "    data_index = [False for i in range(data_size)]\n",
    "    sampling = int((int(data_size/n_sample) * 0.25))\n",
    "    labels = data.columns\n",
    "    \n",
    "    temp_idx = [] \n",
    "    final_data = pd.DataFrame(data,columns=labels)\n",
    "    final_data.reset_index(drop=True)\n",
    "    \n",
    "    #find the number of bearing differance above the turn sensitivity\n",
    "    bearing_diff , all_distances = dist_and_bearing_diff(final_data)\n",
    "    mean_dist = mean(all_distances)\n",
    "    for idx, x in enumerate(bearing_diff):\n",
    "        sampling_acc = check_sampling(x,sampling,data_index)\n",
    "        if (x[0] > turn_sensitivity) and (all_distances[idx] > mean_dist/2) and sampling_acc:\n",
    "            data_index[x[1]] = True\n",
    "            temp_idx.append(x[1])\n",
    "    while n_sample > len(temp_idx):\n",
    "        final_index=fitting_indexes(temp_idx,n_sample)\n",
    "\n",
    "    for x in temp_idx:\n",
    "        data_index[x]=True\n",
    "    \n",
    "    return final_data[data_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x_train=angle_diff(x_train)\n",
    "#x_test=angle_diff(x_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File b'/home/kapadais/Desktop/HUA Thesis/ptixiaki hua/data/route.csv' does not exist: b'/home/kapadais/Desktop/HUA Thesis/ptixiaki hua/data/route.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-c73b9f018ebb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"TIMESTAMP\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"LAT\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"LON\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"HEADING\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mreal_data\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/home/kapadais/Desktop/HUA Thesis/ptixiaki hua/data/route.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mreal_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreal_data\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mreal_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'TIMESTAMP'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mreal_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreal_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/gendis_test/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    700\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    701\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 702\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    703\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/gendis_test/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    427\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 429\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    431\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/gendis_test/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/gendis_test/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1120\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1121\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'c'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1122\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1123\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1124\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'python'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/gendis_test/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1851\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'usecols'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1852\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1853\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1854\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1855\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] File b'/home/kapadais/Desktop/HUA Thesis/ptixiaki hua/data/route.csv' does not exist: b'/home/kapadais/Desktop/HUA Thesis/ptixiaki hua/data/route.csv'"
     ]
    }
   ],
   "source": [
    "labels = [\"TIMESTAMP\",\"LAT\",\"LON\",\"HEADING\"]\n",
    "real_data =  pd.read_csv(\"/home/kotsos/Desktop/hua/route.csv\")\n",
    "real_data = real_data [labels]\n",
    "real_data.sort_values('TIMESTAMP',inplace=True)\n",
    "real_data=real_data.reset_index(drop=True)\n",
    "data = scalling_down_windowed(real_data,train_test_options[\"split\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = np.array([0,1])\n",
    "print(y_test.shape)\n",
    "a=np.array(data[\"HEADING\"].values).astype(int)\n",
    "x_test=np.array([a,x_test[1]])\n",
    "print(x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Visualize the timeseries in the train and test set\n",
    "# colors = ['r', 'b', 'g', 'y', 'c']\n",
    "# plt.figure(figsize=(20, 10))\n",
    "# for ts, label in zip(x_train, y_train):\n",
    "#     plt.plot(range(len(ts)), ts, c=colors[int(label%len(colors))])\n",
    "# plt.title('The timeseries in the train set')\n",
    "# plt.show()\n",
    "\n",
    "# plt.figure(figsize=(20, 10))\n",
    "# for ts, label in zip(x_test, y_test):\n",
    "#     plt.plot(range(len(ts)), ts, c=colors[int(label%len(colors))])\n",
    "# plt.title('The timeseries in the test set')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(\"standardized train and test data\\n\")\n",
    "x_train, x_test = standardize_data(x_train, x_test)\n",
    "genetic_extractor = GeneticExtractor(**genetic_options)\n",
    "print_genetic_param(genetic_extractor)\n",
    "genetic_extractor.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distances_train = genetic_extractor.transform(x_train)\n",
    "distances_test = genetic_extractor.transform(x_test)\n",
    "lr = LogisticRegression()\n",
    "lr.fit(distances_train, y_train)\n",
    "\n",
    "# Print the accuracy score on the test set\n",
    "accuracy_result = accuracy_score(y_test, lr.predict(distances_test))\n",
    "print('Accuracy = {}'.format(accuracy_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr.predict(distances_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
