{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "import glob\n",
    "import shutil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from rdp import rdp\n",
    "from statistics import mean\n",
    "from random import choice\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.externals import joblib\n",
    "from sklearn import preprocessing\n",
    "from gendis.genetic import GeneticExtractor\n",
    "from tools.utils import scale_down, standardize_data, set_movements, angle_diff, get_distance\n",
    "\n",
    "train_test_options = {\"split\": 25}\n",
    "movement_list = [\"step_up_left\",\"step_up_right\",\n",
    "                 \"spiral_movement_left\",\"spiral_movement_right\",\n",
    "                 \"expanding_square_left\",\"expanding_square_right\",\n",
    "                 \"creeping_line_left\",\"creeping_line_right\",\n",
    "                 \"sector_pattern_left\",\"sector_pattern_right\"]\n",
    "\n",
    "def fig_dir_creation(path,reset=False):\n",
    "    if not reset :\n",
    "        try:\n",
    "            os.mkdir(path)\n",
    "        except OSError:  \n",
    "            print (\"Creation of the directory %s failed\" % path)\n",
    "    else:\n",
    "        if len(glob.glob(path))==0:\n",
    "            os.mkdir(path)\n",
    "        else:\n",
    "            shutil.rmtree(path)\n",
    "            os.mkdir(path)\n",
    "\n",
    "def experiments(ship,movement_list,type_exp=\"\",mixed_models=False):\n",
    "    predictions = []\n",
    "    proba = []\n",
    "    type_exp =  \"_\"+type_exp if type_exp == \"anglediff\" else \"\"\n",
    "    if not mixed_models:\n",
    "        for x in movement_list:\n",
    "            gen_ext = 0\n",
    "            lr = 0\n",
    "            with open(\"models/\"+x+type_exp+\"_genetic_extractor.pkl\", 'rb') as pickle_file:\n",
    "                gen_ext = joblib.load( pickle_file)\n",
    "                pickle_file.close()\n",
    "            with open(\"models/\"+x+type_exp+\"_logistic_regression.pkl\",'rb') as pickle_file:\n",
    "                lr = joblib.load(pickle_file)\n",
    "                pickle_file.close()\n",
    "            x_test = ship[\"data\"]\n",
    "            x_test = preprocessing.scale(x_test,axis=1)\n",
    "            distances_test = gen_ext.transform(x_test)\n",
    "            predictions.append(lr.predict(distances_test))\n",
    "            proba.append(lr.predict_proba(distances_test))\n",
    "    else:\n",
    "        for x in movement_list:\n",
    "            gen_ext = 0\n",
    "            lr = 0\n",
    "            gen_ext_anglediff = 0\n",
    "            lr_angle_diff = 0\n",
    "            type_exp_normal = \"\"\n",
    "            type_exp_anglediff = \"_anglediff\"\n",
    "            type_exp_mixed =\"_mixed\"\n",
    "            with open(\"models/\"+x+type_exp_normal+\"_genetic_extractor.pkl\", 'rb') as pickle_file:\n",
    "                gen_ext = joblib.load( pickle_file)\n",
    "                pickle_file.close()\n",
    "            with open(\"models/\"+x+type_exp_normal+\"_logistic_regression.pkl\",'rb') as pickle_file:\n",
    "                lr = joblib.load(pickle_file)\n",
    "                pickle_file.close()\n",
    "            with open(\"models/\"+x+type_exp_anglediff+\"_genetic_extractor.pkl\", 'rb') as pickle_file:\n",
    "                gen_ext_anglediff = joblib.load( pickle_file)\n",
    "                pickle_file.close()\n",
    "            with open(\"models/\"+x+type_exp_anglediff+\"_logistic_regression.pkl\",'rb') as pickle_file:\n",
    "                lr_angle_diff = joblib.load(pickle_file)\n",
    "                pickle_file.close()\n",
    "            with open(\"models/\"+x+type_exp_mixed+\"_logistic_regression.pkl\",'rb') as pickle_file:\n",
    "                lr_mixed = joblib.load(pickle_file)\n",
    "                pickle_file.close()\n",
    "            \n",
    "            x_test = ship[\"data\"]\n",
    "            x_test_anglediff = angle_diff(ship[\"data\"])\n",
    "            x_test = preprocessing.scale(x_test,axis=1)\n",
    "            x_test_anglediff = preprocessing.scale(x_test_anglediff,axis=1)\n",
    "            if x_test.shape[0] == 1:\n",
    "                b = x_test[1].reshape(1,-1)\n",
    "                b = preprocessing.normalize(b,axis=1)\n",
    "                x_test = np.array([b]).reshape(1,25)\n",
    "            x_test = preprocessing.normalize(x_test,axis=1)\n",
    "            if x_test_anglediff.shape[0] == 1:\n",
    "                b = x_test_anglediff[1].reshape(1,-1)\n",
    "                b = preprocessing.normalize(b,axis=1)\n",
    "                x_test_anglediff = np.array([b]).reshape(1,25)\n",
    "            x_test_anglediff = preprocessing.normalize(x_test,axis=1)\n",
    "            \n",
    "            distances_test = gen_ext.transform(x_test)\n",
    "            distances_test_anglediff = gen_ext_anglediff.transform(x_test_anglediff)\n",
    "            \n",
    "            mixed_distances = np.concatenate((distances_test,distances_test_anglediff),axis=1) \n",
    "            \n",
    "            predictions.append(lr_mixed.predict(mixed_distances))\n",
    "            proba.append(lr_mixed.predict_proba(mixed_distances))\n",
    "    return predictions,proba\n",
    "\n",
    "def results(movement_list,proba,predictions,proba_filter=False):\n",
    "    proba_info = []\n",
    "    pattern_indexes = []\n",
    "    for idx,x in enumerate(movement_list):\n",
    "        count = 0\n",
    "        indexes = []\n",
    "        for ind,i in enumerate(predictions[idx]):\n",
    "            if proba_filter:\n",
    "                if i ==0 and proba[idx][ind][0] > 0.75:\n",
    "                    count = count + 1\n",
    "                    proba_info.append({\"pattern\":x,\"first_class\":proba[idx][ind][0],\"second_class\":proba[idx][ind][1],\"chunk\":ind})\n",
    "                    indexes.append(ind)\n",
    "                else:\n",
    "                    if i ==0:\n",
    "                        count = count + 1\n",
    "                        proba_info.append({\"pattern\":x,\"first_class\":proba[idx][ind][0],\"second_class\":proba[idx][ind][1],\"chunk\":ind})\n",
    "                        indexes.append(ind)\n",
    "        print(count,\"times of pattern detection:\",x,\" at indexes \",indexes)\n",
    "        pattern_indexes.append(indexes)\n",
    "    return proba_info,pattern_indexes\n",
    "\n",
    "\n",
    "\n",
    "def find_best_patterns(proba_info):\n",
    "    a=pd.DataFrame(proba_info,columns=[\"pattern\",\"chunk\",\"first_class\",\"second_class\"])\n",
    "    a.set_index(\"chunk\",inplace=True)\n",
    "    grouped_df = a[a.groupby(\"chunk\").count()[\"pattern\"]>1].groupby(\"chunk\")\n",
    "    best_patterns = []\n",
    "    for key, item in grouped_df:\n",
    "        temp = item[grouped_df.get_group(key).loc[key][\"first_class\"]==grouped_df.get_group(key).max()[\"first_class\"]]\n",
    "        best_patterns.append([key,temp.loc[key][\"pattern\"]])\n",
    "    for x in best_patterns:\n",
    "        print(\"For chunk no:\",x[0],\"the best pattern is \",x[1])   \n",
    "    return best_patterns\n",
    "\n",
    "def plots(movement_list,\n",
    "          pattern_indexes,\n",
    "          best_patterns,\n",
    "          ship_name,\n",
    "          ship_dfs,\n",
    "          path,\n",
    "          file=\"normal\",\n",
    "          save=True,\n",
    "          scaled_down=True,\n",
    "          rdp_data=None):\n",
    "    chunks = []\n",
    "    patterns = []\n",
    "    for x in best_patterns:\n",
    "        chunks.append(x[0])\n",
    "        patterns.append(x[1])\n",
    "    for idx,x in enumerate(movement_list):\n",
    "        experiment = file+\"\\n\"+x\n",
    "        for ind,i in enumerate(pattern_indexes[idx]):\n",
    "            if i in chunks:\n",
    "                if(patterns[chunks.index(i)] == x):\n",
    "                    plot_trajectory(ship_dfs[i],\n",
    "                                    train_test_options[\"split\"],\n",
    "                                    i,\n",
    "                                    ship_name,\n",
    "                                    scaled_down,\n",
    "                                    rdp_data,\n",
    "                                    experiment,save=save,path=path)\n",
    "\n",
    "            else:\n",
    "                plot_trajectory(ship_dfs[i],train_test_options[\"split\"],\n",
    "                                i,\n",
    "                                ship_name,\n",
    "                                scaled_down,\n",
    "                                rdp_data,\n",
    "                                experiment,save=save,path=path)\n",
    "\n",
    "def epsilon_selection(real_data_len,data,size,epsilon=0):\n",
    "    if real_data_len<=size:\n",
    "        return False\n",
    "    else:\n",
    "        epsilon = epsilon\n",
    "        i = 0 \n",
    "        mask_len = real_data_len\n",
    "        while mask_len!=size:\n",
    "#             print(\"i:\",i,mask_len,size,epsilon)\n",
    "            mask = rdp(data,algo=\"iter\", return_mask=True,epsilon=epsilon)\n",
    "            mask_len = mask.count(True)\n",
    "            if i >200:\n",
    "                return False\n",
    "            if mask_len < size:\n",
    "                if size - mask_len <=1:\n",
    "                    epsilon = epsilon - 0.0000000005\n",
    "                else :\n",
    "                    epsilon = epsilon - 0.000005\n",
    "            else:\n",
    "                if mask_len -size <=1:\n",
    "                    epsilon = epsilon + 0.00000002\n",
    "                else:\n",
    "                    epsilon = epsilon + 0.000002\n",
    "            i = i +1\n",
    "        return mask\n",
    "\n",
    "def print_results(movement_list,predictions,proba):\n",
    "    for id_pat,patt in enumerate(movement_list):\n",
    "        for id_pre,pred in enumerate(predictions[id_pat]):\n",
    "            print(\"for chunk {2} and pattern {0} a prediction is made with probability {1}\".format(patt,proba[id_pat][id_pre][0],id_pre)) if pred ==0 else None\n",
    "\n",
    "def plot_trajectory(data,\n",
    "                    split,\n",
    "                    chunk_no,\n",
    "                    shipname,\n",
    "                    scaled_down=True,\n",
    "                    rdp_data=None,\n",
    "                    experiment = \"\",\n",
    "                    save=False,\n",
    "                    path=\"\"):\n",
    "    df_list = []\n",
    "    data_list = []\n",
    "    real_data=data.assign(Trajectory=\"Real Trajectory\")\n",
    "    df_list.append(real_data)\n",
    "    data_list.append(real_data[\"Trajectory\"].iloc[0])\n",
    "    pallete = ['r']\n",
    "    sizes = [6]\n",
    "    dashes = [(5,5)]\n",
    "    markers = [\"^\"]\n",
    "    if scaled_down:         \n",
    "        data_scaled=scale_down(data,split).assign(Trajectory=\"Scaled Trajectory\")\n",
    "        if len(data_scaled) == split:\n",
    "            df_list.append(data_scaled)\n",
    "            data_list.append(data_scaled[\"Trajectory\"].iloc[0])\n",
    "            pallete.append('b')\n",
    "            sizes.append(3)\n",
    "            dashes.append(\"\")\n",
    "            markers.append(\"o\")\n",
    "\n",
    "    \n",
    "    if rdp_data is not None and type(rdp_data) == type(pd.DataFrame()) and len(rdp_data)>0:\n",
    "        rdp_data=data.assign(Trajectory=\"RDP Trajectory\")\n",
    "        df_list.append(rdp_data)\n",
    "        data_list.append(rdp_data[\"Trajectory\"].iloc[0])\n",
    "        pallete.append('g')\n",
    "        sizes.append(1)\n",
    "        dashes.append((5, 10))\n",
    "        markers.append(\"o\")\n",
    "\n",
    "    merged_data = pd.concat(df_list)\n",
    "\n",
    "    fig, ax= plt.subplots(1, 1, figsize=(30, 20),sharex=True)    \n",
    "    start = datetime.strftime(data[\"TIMESTAMP\"].head(1).iloc[0],'%Y-%m-%d %H:%M:%S')\n",
    "    end = datetime.strftime(data[\"TIMESTAMP\"].tail(1).iloc[0],'%Y-%m-%d %H:%M:%S')\n",
    "    title = \"Vessel:\"+shipname+\"\\nClass:\"+experiment+\"\\nCHUNK NO: \"+str(chunk_no)+\"\\n\"+start+\"----\"+end\n",
    "    ax.set_title(title,fontsize=20)\n",
    "    sns.lineplot(x='LON', y='LAT',\n",
    "                      data=merged_data,\n",
    "                      ax=ax,\n",
    "                      sort=False,\n",
    "                      hue = \"Trajectory\",\n",
    "                      palette = pallete,\n",
    "                      hue_order= data_list,\n",
    "                      size=merged_data[\"Trajectory\"],\n",
    "                      sizes = sizes,\n",
    "                      size_order = data_list,\n",
    "                      style = merged_data[\"Trajectory\"],\n",
    "                      style_order = data_list,\n",
    "                      dashes = dashes,\n",
    "                      markers = markers,\n",
    "                      legend =\"full\",\n",
    "                     )\n",
    "    lgnd = plt.legend(loc=\"lower left\", numpoints=1, fontsize=25)\n",
    "    ax.set_ylabel('Latitute', fontsize = 20.0) # Y label\n",
    "    ax.set_xlabel('Longitude ', fontsize = 20.0) # X label\n",
    "    plt.savefig(path+\"/chunk_no_\"+str(chunk_no)+\".png\",dpi=fig.dpi) if save and path!=\"\" else None\n",
    "    \n",
    "def read_ship_csv(file):\n",
    "    ship =  pd.read_csv(file)\n",
    "    shipname = ship.loc[0][\"SHIPNAME\"]\n",
    "    ship = ship [[\"TIMESTAMP\",\"LAT\",\"LON\",\"HEADING\"]]\n",
    "    ship['TIMESTAMP'] = pd.to_datetime(ship['TIMESTAMP'])  \n",
    "    ship.sort_values('TIMESTAMP',inplace=True)\n",
    "    ship=ship.reset_index(drop=True)\n",
    "    return ship,shipname\n",
    "\n",
    "def get_scaled_down_dfs(ship_dfs):\n",
    "    ship_data_chunked = []\n",
    "    ship_data_chunked_index = [False for i in range(0,len(ship_dfs))]\n",
    "    for idx,x in enumerate(ship_dfs):\n",
    "        x = scale_down(x,train_test_options[\"split\"])\n",
    "        x = np.array(x[\"HEADING\"].values.astype(int))\n",
    "        if len(x) == train_test_options[\"split\"]:\n",
    "            ship_data_chunked_index[idx] = True\n",
    "            ship_data_chunked.append(x)\n",
    "    ship_data_chunked = np.array(ship_data_chunked)\n",
    "    ship_dfs = [ship for (ship,index) in zip(ship_dfs,ship_data_chunked_index) if  index ]\n",
    "    return ship_data_chunked,ship_dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = \"figures/optimization\"\n",
    "fig_dir_creation(base_path,reset=True)\n",
    "labels = [\"TIMESTAMP\",\"LAT\",\"LON\",\"HEADING\"]\n",
    "real_data =  pd.read_csv(\"../data/route.csv\")\n",
    "real_data = real_data [labels][:500]\n",
    "real_data['TIMESTAMP'] = pd.to_datetime(real_data['TIMESTAMP'])\n",
    "real_data.sort_values('TIMESTAMP',inplace=True)\n",
    "real_data=real_data.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>First case :</b> check for a known case of pattern (creeping_line_left for SIEM PILOT ship) if the result of classification is right with the use of normal settings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.mkdir(base_path+\"/first_case\")\n",
    "x = scale_down(real_data,train_test_options[\"split\"])\n",
    "x = np.array([x[\"HEADING\"].values.astype(int)])\n",
    "ship = {\"shipname\":\"SIEM PILOT\",\"data\":x,\"ship_dfs\":[real_data]}\n",
    "predictions,proba = experiments(ship,movement_list)\n",
    "print_results(movement_list,predictions,proba)\n",
    "proba_info,pattern_indexes = results(movement_list,proba,predictions,proba_filter=True)\n",
    "best_patterns = find_best_patterns(proba_info)\n",
    "plots(movement_list=movement_list,\n",
    "      pattern_indexes=pattern_indexes,\n",
    "      best_patterns=best_patterns,\n",
    "      ship_name=ship[\"shipname\"],\n",
    "      ship_dfs=ship[\"ship_dfs\"],path=base_path+\"/first_case\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Second case :</b> check for a known case of pattern (creeping_line_left for SIEM PILOT ship) if the result of classification is right with the use of rdp algorithm before the scale_down section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.mkdir(base_path+\"/second_case\")\n",
    "lat = np.array(real_data[\"LAT\"])\n",
    "lon = np.array(real_data[\"LON\"])\n",
    "np_real_data = [np.array([x,lon[idx]]) for idx,x in enumerate(lat)]\n",
    "mask = rdp(np_real_data,algo=\"iter\", return_mask=True,epsilon=0.005)\n",
    "rdp_real_data =  real_data[mask]\n",
    "if len(rdp_real_data) == train_test_options[\"split\"]:\n",
    "    rdp_x = scale_down(rdp_real_data,train_test_options[\"split\"])\n",
    "    rdp_x = np.array([rdp_x[\"HEADING\"].values.astype(int)])\n",
    "    rdp_ship = {\"shipname\":\"SIEM PILOT\",\"data\":rdp_x,\"ship_dfs\":[rdp_real_data]}\n",
    "    rdp_predictions,rdp_proba = experiments(rdp_ship,movement_list)\n",
    "    print_results(movement_list,rdp_predictions,rdp_proba)\n",
    "    rdp_proba_info,rdp_pattern_indexes = results(movement_list,rdp_proba,rdp_predictions,proba_filter=True)\n",
    "    rdp_best_patterns = find_best_patterns(rdp_proba_info)\n",
    "    plots(movement_list=movement_list,\n",
    "          pattern_indexes=rdp_pattern_indexes,\n",
    "          best_patterns=rdp_best_patterns,\n",
    "          ship_name=rdp_ship[\"shipname\"],\n",
    "          ship_dfs=[real_data],\n",
    "          scaled_down=True,\n",
    "          rdp_data=rdp_real_data,path=base_path+\"/second_case\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Third case :</b> check for a known case of pattern (creeping_line_left for SIEM PILOT ship) if the result of classification is right with chunks with size 180-sample(50-sample size are too abstract to pass scale-down) (from a trajectory with size 500-sample) using scale-down (found only 3 suitable chunk after scale down )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "os.mkdir(base_path+\"/third_case\")\n",
    "n = 180  #chunk row size\n",
    "small_ship_dfs = [real_data[i:i+n] for i in range(0,real_data.shape[0],n)]\n",
    "small_ship_data_chunked,small_ship_dfs = get_scaled_down_dfs(small_ship_dfs)\n",
    "small_ship ={\"shipname\":\"SIEM PILOT\",\"data\":small_ship_data_chunked,\"ship_dfs\":small_ship_dfs}\n",
    "print(\"ERROR DIFFERENT SIZE OF DATA\") if len(small_ship_dfs)!=small_ship[\"data\"].shape[0] else None\n",
    "\n",
    "small_predictions,small_proba = experiments(small_ship,movement_list)\n",
    "print_results(movement_list,small_predictions,small_proba)\n",
    "small_proba_info,small_pattern_indexes = results(movement_list,small_proba,small_predictions,proba_filter=True)\n",
    "small_best_patterns = find_best_patterns(small_proba_info)\n",
    "plots(movement_list=movement_list,\n",
    "      pattern_indexes=small_pattern_indexes,\n",
    "      best_patterns=small_best_patterns,\n",
    "      ship_name=small_ship[\"shipname\"],\n",
    "      ship_dfs=small_ship[\"ship_dfs\"],path=base_path+\"/third_case\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "50-sample sized chunk plots of the  of the know SIEM PILOT's trajectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_ship_dfs_chunk = [real_data[i:i+n] for i in range(0,real_data.shape[0],n)]\n",
    "for p_ship in small_ship_dfs_chunk:\n",
    "    fig, ax = plt.subplots()\n",
    "    sns.lineplot(x='LON', y='LAT', data=p_ship, ax=ax,sort=False,marker=\"o\")\n",
    "    fig.set_size_inches(30, 20)\n",
    "    ax.set_ylabel('Latitute', fontsize = 20.0) # Y label\n",
    "    ax.set_xlabel('Longitude ', fontsize = 20.0) # X label\n",
    "    start = datetime.strftime(p_ship[\"TIMESTAMP\"].head(1).iloc[0],'%Y-%m-%d %H:%M:%S')\n",
    "    end = datetime.strftime(p_ship[\"TIMESTAMP\"].tail(1).iloc[0],'%Y-%m-%d %H:%M:%S')\n",
    "    title = \"Time\\n\"+start+\"----\"+end\n",
    "    ax.set_title(title,fontsize = 20.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Fourth case:</b> rerun of third case with rdp algorithm without scale down. The epsilon parameter is selected after extensive search in order to give 25-sample size trajectories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.mkdir(base_path+\"/fourth_case\")\n",
    "n = 180  #chunk row size\n",
    "small_ship_dfs = [real_data[i:i+n] for i in range(0,real_data.shape[0],n)]\n",
    "small_ship_data_chunked = []\n",
    "small_ship_data_chunked_index = [False for i in range(0,len(small_ship_dfs))]  \n",
    "\n",
    "for idx,small_x in enumerate(small_ship_dfs):\n",
    "    small_lat = np.array(small_x[\"LAT\"])\n",
    "    small_lon = np.array(small_x[\"LON\"])\n",
    "    small_np_data= [np.array([x,small_lon[idx]]) for idx,x in enumerate(small_lat)]\n",
    "    mask = epsilon_selection(len(small_x),small_np_data,train_test_options[\"split\"])\n",
    "    if mask:\n",
    "        small_rdp_x =  small_x[mask]\n",
    "        if len(small_rdp_x) == train_test_options[\"split\"]:\n",
    "            small_rdp_x = np.array(small_rdp_x[\"HEADING\"].values.astype(int))\n",
    "            small_ship_data_chunked_index[idx] = True\n",
    "            small_ship_data_chunked.append(small_rdp_x)\n",
    "\n",
    "\n",
    "small_ship_data_chunked = np.array(small_ship_data_chunked)\n",
    "small_ship_dfs = [small_ship for (small_ship,index) in zip(small_ship_dfs,small_ship_data_chunked_index) if  index ]\n",
    "small_rdp_ship ={\"shipname\":\"SIEM PILOT\",\"data\":small_ship_data_chunked,\"ship_dfs\":small_ship_dfs}\n",
    "print(\"ERROR DIFFERENT SIZE OF DATA\") if len(small_ship_dfs)!=small_rdp_ship[\"data\"].shape[0] else None\n",
    "\n",
    "small_rdp_predictions,small_rdp_proba = experiments(small_rdp_ship,movement_list)\n",
    "print_results(movement_list,small_rdp_predictions,small_rdp_proba)\n",
    "small_rdp_proba_info,small_rdp_pattern_indexes = results(movement_list,\n",
    "                                                         small_rdp_proba,\n",
    "                                                         small_rdp_predictions,\n",
    "                                                         proba_filter=True)\n",
    "small_rdp_best_patterns = find_best_patterns(small_rdp_proba_info)\n",
    "plots(movement_list=movement_list,\n",
    "      pattern_indexes=small_rdp_pattern_indexes,\n",
    "      best_patterns=small_rdp_best_patterns,\n",
    "      ship_name=small_rdp_ship[\"shipname\"],\n",
    "      ship_dfs=small_rdp_ship[\"ship_dfs\"],\n",
    "      scaled_down=False,path=base_path+\"/fourth_case\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Rerun</b> all above cases with the appropriate chunks that real_data_experiment marked as chunks classified to <b>a pattern and have some significance</b> (chunks with index :,1,3,5,16,23,27,33 after the scale down)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ship,shipname=read_ship_csv(\"ships/SIEM PILOT.csv\")\n",
    "n = 500  #chunk row size\n",
    "ship_dfs = [ship[i:i+n] for i in range(0,ship.shape[0],n)]  \n",
    "ship_data_chunked, ship_dfs = get_scaled_down_dfs(ship_dfs)\n",
    "selected_trajectories = [False for i in range(0,len(ship_dfs))]\n",
    "for j,i in enumerate(ship_dfs):\n",
    "    if j in [1,4,5,9]:\n",
    "        selected_trajectories[j] = True\n",
    "ship_dfs = [ship for (ship,index) in zip(ship_dfs,selected_trajectories) if  index ]\n",
    "ship_data_chunked = np.array([ship for (ship,index) in zip(ship_data_chunked,selected_trajectories) if  index ])\n",
    "ship ={\"shipname\":shipname,\"data\":ship_data_chunked,\"ship_dfs\":ship_dfs}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div><b><i>SIEM PILOT full trajectory:</i></b>\n",
    "    <ul><b>First case :</b> check for a known case of pattern (creeping_line_left for SIEM PILOT ship) if the result of classification is right with the use of normal settings</ul> </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "os.mkdir(base_path+\"/siem_pilot_full_first_case\")\n",
    "predictions,proba = experiments(ship,movement_list)\n",
    "print_results(movement_list,predictions,proba)\n",
    "proba_info,pattern_indexes = results(movement_list,proba,predictions,proba_filter=True)\n",
    "best_patterns = find_best_patterns(proba_info)\n",
    "plots(movement_list=movement_list,\n",
    "      pattern_indexes=pattern_indexes,\n",
    "      best_patterns=best_patterns,\n",
    "      ship_name=ship[\"shipname\"],\n",
    "      ship_dfs=ship[\"ship_dfs\"],path=base_path+\"/siem_pilot_full_first_case\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div><b><i>SIEM PILOT full trajectory:</i></b>\n",
    "    <ul><b>Second case :</b> check for a known case of pattern (creeping_line_left for SIEM PILOT ship) if the result of classification is right with the use of rdp algorithm before the scale_down section</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "os.mkdir(base_path+\"/siem_pilot_full_second_case\")\n",
    "\n",
    "for x in ship_dfs:\n",
    "    print(\"Chunk from {0} to {1}\".format(x.index[0],x.index[-1]))\n",
    "    lat = np.array(x[\"LAT\"])\n",
    "    lon = np.array(x[\"LON\"])\n",
    "    np_real_data = [np.array([x_lat,lon[idx]]) for idx,x_lat in enumerate(lat)]\n",
    "    mask = rdp(np_real_data,algo=\"iter\", return_mask=True,epsilon=0.000009)\n",
    "    rdp_real_data =  x[mask]\n",
    "    print(\"Size of chunk after rdp:\",len(rdp_real_data))\n",
    "    if len(rdp_real_data) > train_test_options[\"split\"]:\n",
    "        rdp_x = scale_down(rdp_real_data,train_test_options[\"split\"])\n",
    "        if len(rdp_x)==train_test_options[\"split\"]:\n",
    "            print(\"scale down successful\")\n",
    "            rdp_x = np.array([rdp_real_data[\"HEADING\"].values.astype(int)])\n",
    "            rdp_ship = {\"shipname\":\"SIEM PILOT\",\"data\":rdp_x,\"ship_dfs\":[rdp_real_data]}\n",
    "            rdp_predictions,rdp_proba = experiments(rdp_ship,movement_list)\n",
    "            rdp_proba_info,rdp_pattern_indexes = results(movement_list,rdp_proba,rdp_predictions,proba_filter=True)\n",
    "            rdp_best_patterns = find_best_patterns(rdp_proba_info)\n",
    "            plots(movement_list=movement_list,\n",
    "                  pattern_indexes=rdp_pattern_indexes,\n",
    "                  best_patterns=rdp_best_patterns,\n",
    "                  ship_name=rdp_ship[\"shipname\"],\n",
    "                  ship_dfs=[x],\n",
    "                  rdp_data=rdp_real_data,path=base_path+\"/siem_pilot_full_second_case\"\n",
    "                 )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div><b><i>SIEM PILOT full trajectory:</i></b>\n",
    "    <ul><b>Third case :</b> check for a known case of pattern (creeping_line_left for SIEM PILOT ship) if the result of classification is right with chunks with size 180-sample(50-sample size are too abstract to pass scale-down) (from a trajectory with size 500-sample) using scale-down </ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "os.mkdir(base_path+\"/siem_pilot_full_third_case\")\n",
    "for x in ship_dfs:\n",
    "    print(\"Chunk from {0} to {1}\".format(x.index[0],x.index[-1]))\n",
    "    n = 180  #chunk row size\n",
    "    small_ship_dfs = [x[i:i+n] for i in range(0,real_data.shape[0],n)]\n",
    "    small_ship_data_chunked ,small_ship_dfs = get_scaled_down_dfs(small_ship_dfs)\n",
    "    small_ship ={\"shipname\":\"SIEM PILOT\",\"data\":small_ship_data_chunked,\"ship_dfs\":small_ship_dfs}\n",
    "    print(\"ERROR DIFFERENT SIZE OF DATA\") if len(small_ship_dfs)!=small_ship[\"data\"].shape[0] else None\n",
    "\n",
    "    small_predictions,small_proba = experiments(small_ship,movement_list)\n",
    "    print_results(movement_list,small_predictions,small_proba)\n",
    "    small_proba_info,small_pattern_indexes = results(movement_list,small_proba,small_predictions,proba_filter=True)\n",
    "    small_best_patterns = find_best_patterns(small_proba_info)\n",
    "    plots(movement_list=movement_list,\n",
    "          pattern_indexes=small_pattern_indexes,\n",
    "          best_patterns=small_best_patterns,\n",
    "          ship_name=small_ship[\"shipname\"],\n",
    "          ship_dfs=small_ship[\"ship_dfs\"],path=base_path+\"/siem_pilot_full_third_case\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div><b><i>SIEM PILOT full trajectory:</i></b>\n",
    "    <ul><b>Final  case :</b> check for a known case of pattern (creeping_line_left for SIEM PILOT ship) if the result of classification is right but with the use of distances from normal models and angle_diff models  </ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.mkdir(base_path+\"/siem_pilot_full_mixed_models_case\")\n",
    "predictions,proba = experiments(ship,movement_list,mixed_models=True)\n",
    "print_results(movement_list,predictions,proba)\n",
    "proba_info,pattern_indexes = results(movement_list,proba,predictions,proba_filter=True)\n",
    "best_patterns = find_best_patterns(proba_info)\n",
    "plots(movement_list=movement_list,\n",
    "      pattern_indexes=pattern_indexes,\n",
    "      best_patterns=best_patterns,\n",
    "      ship_name=ship[\"shipname\"],\n",
    "      ship_dfs=ship[\"ship_dfs\"],path=base_path+\"/siem_pilot_full_mixed_models_case\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
