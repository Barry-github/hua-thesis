{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tools.utils import scale_down\n",
    "from tools.utils import standardize_data, print_genetic_param, print_settings, set_movements, angle_diff, get_distance\n",
    "from statistics import mean\n",
    "from random import choice\n",
    "train_test_options = {\"split\": 6}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_sampling(x,sampling,data_index):\n",
    "    i = 1\n",
    "    sampling_acc = True\n",
    "    down_limit = (x[1]-sampling)\n",
    "    up_limit = (x[1]+sampling)\n",
    "    while i<=sampling:\n",
    "        if 0 <=down_limit and up_limit < len(data_index):\n",
    "            if (data_index[x[1]-i] == True) or (data_index[x[1]+i] == True):\n",
    "                sampling_acc = False\n",
    "                break\n",
    "        i = i + 1      \n",
    "    return sampling_acc\n",
    "\n",
    "\n",
    "def dist_and_bearing_diff(data):\n",
    "    all_distances = []\n",
    "    bearing_diff = []\n",
    "    data_size=len(data)\n",
    "    i = 0\n",
    "    while i<data_size:\n",
    "        if i + 1 >=data_size:\n",
    "            break\n",
    "        bearing_1, bearing_2 = data[\"HEADING\"].iloc[i], data[\"HEADING\"].iloc[i+1] \n",
    "        bearing_diff.append([abs(bearing_2 - bearing_1),i])\n",
    "        lat_1, lon_1, lat_2, lon_2 = data[\"LAT\"].iloc[i], data[\"LON\"].iloc[i], data[\"LAT\"].iloc[i+1], data[\"LON\"].iloc[i+1]\n",
    "        all_distances.append(get_distance(lat_1,lon_1,lat_2,lon_2))\n",
    "        i = i +1\n",
    "    return bearing_diff, all_distances \n",
    "\n",
    "\n",
    "def fitting_indexes(arr,new_size):\n",
    "    i = 0\n",
    "    r_arr = []\n",
    "    while i <len(arr):\n",
    "        if i+1 >=len(arr):\n",
    "            break\n",
    "        r_arr.append(arr[i+1]-arr[i])\n",
    "        i = i + 1\n",
    "    if len(r_arr) <= 0:\n",
    "        return arr\n",
    "    mean_space = mean(r_arr)\n",
    "    i = 0 \n",
    "    while i<len(arr)<new_size:\n",
    "        if i+1 >=len(arr):\n",
    "            break\n",
    "        diff = arr[i+1]-arr[i]\n",
    "        if mean_space <= diff:\n",
    "            step = int(diff/2)\n",
    "            new_index = arr[i] + step\n",
    "            arr.insert(i+1,new_index)\n",
    "            i = i + 1\n",
    "        i = i + 1\n",
    "    return arr\n",
    "\n",
    "def scalling_down_windowed(data,n_sample,turn_sensitivity=30):\n",
    "    if len(data) <= n_sample :\n",
    "        return data\n",
    "    size_correction = int(len(data) / n_sample) * n_sample\n",
    "    data=data[:size_correction]\n",
    "    data_size=len(data)\n",
    "    data_index = [False for i in range(data_size)]\n",
    "    sampling = int((int(data_size/n_sample) * 0.25))\n",
    "    labels = data.columns\n",
    "    \n",
    "    temp_idx = [] \n",
    "    final_data = pd.DataFrame(data,columns=labels)\n",
    "    final_data.reset_index(drop=True)\n",
    "    \n",
    "    #find the number of bearing differance above the turn sensitivity\n",
    "    bearing_diff , all_distances = dist_and_bearing_diff(final_data)\n",
    "    mean_dist = mean(all_distances)\n",
    "    for idx, x in enumerate(bearing_diff):\n",
    "        sampling_acc = check_sampling(x,sampling,data_index)\n",
    "        if (x[0] > turn_sensitivity) and (all_distances[idx] > mean_dist/2) and sampling_acc:\n",
    "            data_index[x[1]] = True\n",
    "            temp_idx.append(x[1])\n",
    "    while n_sample > len(temp_idx) and len(temp_idx) > 1 :\n",
    "        fitting_indexes(temp_idx,n_sample)\n",
    "\n",
    "    for x in temp_idx:\n",
    "        data_index[x]=True\n",
    "        \n",
    "    return final_data[data_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(33, 6)\n"
     ]
    }
   ],
   "source": [
    "ship =  pd.read_csv(\"ships/SIEM PILOT.csv\")\n",
    "shipname = ship.loc[0][\"SHIPNAME\"]\n",
    "ship = ship [[\"TIMESTAMP\",\"LAT\",\"LON\",\"HEADING\"]]\n",
    "ship['TIMESTAMP'] = pd.to_datetime(ship['TIMESTAMP'])  \n",
    "ship.sort_values('TIMESTAMP',inplace=True)\n",
    "ship=ship.reset_index(drop=True)\n",
    "# ship = scalling_down_windowed(ship[0:500],train_test_options[\"split\"])\n",
    "n = 500  #chunk row size\n",
    "ship_dfs = [ship[i:i+n] for i in range(0,ship.shape[0],n)]\n",
    "ship_data_chunked = []\n",
    "for idx,x in enumerate(ship_dfs):\n",
    "    x = scalling_down_windowed(x,train_test_options[\"split\"])\n",
    "    x = np.array(x[\"HEADING\"].values.astype(int))\n",
    "    if len(x) == train_test_options[\"split\"]:\n",
    "        ship_data_chunked.append(x)\n",
    "ship_data_chunked = np.array(ship_data_chunked)\n",
    "ship ={\"shipname\":shipname,\"data\":ship_data_chunked}\n",
    "print(ship[\"data\"].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(glob.glob('ships')) != 0 :\n",
    "    files = glob.glob(\"ships/*.csv\")\n",
    "    all_ships = [] \n",
    "    for file in files:\n",
    "        ship =  pd.read_csv(file)\n",
    "        shipname = ship.loc[0][\"SHIPNAME\"]\n",
    "        ship = ship [[\"TIMESTAMP\",\"LAT\",\"LON\",\"HEADING\"]]\n",
    "        ship['TIMESTAMP'] = pd.to_datetime(ship['TIMESTAMP'])  \n",
    "        ship.sort_values('TIMESTAMP',inplace=True)\n",
    "        ship=ship.reset_index(drop=True)\n",
    "        n = 500  #chunk row size\n",
    "        ship_dfs = [ship[i:i+n] for i in range(0,ship.shape[0],n)]\n",
    "        ship_data_chunked = []\n",
    "        for x in ship_dfs:\n",
    "            x = scale_down(x,train_test_options[\"split\"])\n",
    "            x = np.array(x[\"HEADING\"].values.astype(int))\n",
    "            ship_data_chunked.append(x)\n",
    "        ship_data_chunked = np.array(ship_data_chunked)\n",
    "        ship ={\"shipname\":shipname,\"data\":ship_data_chunked}\n",
    "    all_ships.append(ship)\n",
    "    print(all_ships)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-05-22 23:43:54.631 | INFO     | tools.data_extraction:read_datasets:22 - Reading the data files\n",
      "2019-05-22 23:43:55.008 | SUCCESS  | tools.data_extraction:read_datasets:51 - Done reading files\n",
      "2019-05-22 23:43:55.016 | INFO     | tools.data_extraction:load_datasets:129 - Loading the csv files to the appropriate train and test arrays(nparrays)\n",
      "2019-05-22 23:43:55.033 | SUCCESS  | tools.data_extraction:load_datasets:134 - Done\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<_io.BufferedReader name='step_up_left.pkl'>\n",
      "None\n",
      "Experiment for pattern step_up_left\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'transform'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-87-cd3e90f67e75>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgen_ext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Experiment for pattern\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mdistances_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgen_ext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0mlr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mlr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdistances_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'transform'"
     ]
    }
   ],
   "source": [
    "movement_list = [\"step_up_left\"]\n",
    "# ,\"step_up_right\",\n",
    "#                  \"spiral_movement_right\",\"spiral_movement_left\",\n",
    "#                  \"expanding_square_right\",\"expanding_square_left\",\n",
    "#                  \"creeping_line_left\",\"creeping_line_right\",\n",
    "#                  \"sector_pattern_left\",\"sector_pattern_right\"]\n",
    "from sklearn.externals import joblib\n",
    "from gendis.genetic import GeneticExtractor\n",
    "from tools.data_extraction import DataExtractor\n",
    "dex = DataExtractor()\n",
    "x_train, y_train, x_test, y_test = dex.load_datasets()\n",
    "for x in movement_list:\n",
    "    with open(x+\".pkl\", 'rb') as pickle_file:\n",
    "        print(pickle_file)\n",
    "        gen_ext = joblib.load( pickle_file)\n",
    "        print(gen_ext)\n",
    "        print(\"Experiment for pattern\",x)\n",
    "        distances_train = gen_ext.transform(x_train)\n",
    "        lr = LogisticRegression()\n",
    "        lr.fit(distances_train, y_train)\n",
    "        distances_test = gen_ext.transform(ship[\"data\"])\n",
    "        print(lr.predict(distances_test))\n",
    "        pickle_file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
