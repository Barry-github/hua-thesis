{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "import glob\n",
    "import shutil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from statistics import mean\n",
    "from random import choice\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.externals import joblib\n",
    "from sklearn import preprocessing\n",
    "from gendis.genetic import GeneticExtractor\n",
    "from tools.utils import scale_down, standardize_data, set_movements, angle_diff, get_distance\n",
    "\n",
    "train_test_options = {\"split\": 25}\n",
    "movement_list = [\"step_up_left\",\"step_up_right\",\n",
    "                 \"spiral_movement_left\",\"spiral_movement_right\",\n",
    "                 \"expanding_square_left\",\"expanding_square_right\",\n",
    "                 \"creeping_line_left\",\"creeping_line_right\",\n",
    "                 \"sector_pattern_left\",\"sector_pattern_right\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiments(ship,movement_list,type_exp=\"\",mixed_models=False):\n",
    "    predictions = []\n",
    "    proba = []\n",
    "    type_exp =  \"_\"+type_exp if type_exp == \"anglediff\" else \"\"\n",
    "    if not mixed_models:\n",
    "        for x in movement_list:\n",
    "            gen_ext = 0\n",
    "            lr = 0\n",
    "            with open(\"models/\"+x+type_exp+\"_genetic_extractor.pkl\", 'rb') as pickle_file:\n",
    "                gen_ext = joblib.load( pickle_file)\n",
    "                pickle_file.close()\n",
    "            with open(\"models/\"+x+type_exp+\"_logistic_regression.pkl\",'rb') as pickle_file:\n",
    "                lr = joblib.load(pickle_file)\n",
    "                pickle_file.close()\n",
    "            x_test = ship[\"data\"]\n",
    "            x_test = preprocessing.scale(x_test,axis=1)\n",
    "            if x_test.shape[0] == 1:\n",
    "                b = x_test[1].reshape(1,-1)\n",
    "                b = preprocessing.normalize(b,axis=1)\n",
    "                x_test = np.array([b]).reshape(1,25)\n",
    "            x_test = preprocessing.normalize(x_test,axis=1)\n",
    "            distances_test = gen_ext.transform(x_test)\n",
    "            predictions.append(lr.predict(distances_test))\n",
    "            proba.append(lr.predict_proba(distances_test))\n",
    "    else:\n",
    "        for x in movement_list:\n",
    "            gen_ext = 0\n",
    "            lr = 0\n",
    "            gen_ext_anglediff = 0\n",
    "            lr_angle_diff = 0\n",
    "            type_exp_normal = \"\"\n",
    "            type_exp_anglediff = \"_anglediff\"\n",
    "            type_exp_mixed =\"_mixed\"\n",
    "            with open(\"models/\"+x+type_exp_normal+\"_genetic_extractor.pkl\", 'rb') as pickle_file:\n",
    "                gen_ext = joblib.load( pickle_file)\n",
    "                pickle_file.close()\n",
    "            with open(\"models/\"+x+type_exp_normal+\"_logistic_regression.pkl\",'rb') as pickle_file:\n",
    "                lr = joblib.load(pickle_file)\n",
    "                pickle_file.close()\n",
    "            with open(\"models/\"+x+type_exp_anglediff+\"_genetic_extractor.pkl\", 'rb') as pickle_file:\n",
    "                gen_ext_anglediff = joblib.load( pickle_file)\n",
    "                pickle_file.close()\n",
    "            with open(\"models/\"+x+type_exp_anglediff+\"_logistic_regression.pkl\",'rb') as pickle_file:\n",
    "                lr_angle_diff = joblib.load(pickle_file)\n",
    "                pickle_file.close()\n",
    "            with open(\"models/\"+x+type_exp_mixed+\"_logistic_regression.pkl\",'rb') as pickle_file:\n",
    "                lr_mixed = joblib.load(pickle_file)\n",
    "                pickle_file.close()\n",
    "            \n",
    "            x_test = ship[\"data\"]\n",
    "            x_test_anglediff = angle_diff(ship[\"data\"])\n",
    "            x_test = preprocessing.scale(x_test,axis=1)\n",
    "            x_test_anglediff = preprocessing.scale(x_test_anglediff,axis=1)\n",
    "            \n",
    "            if x_test.shape[0] == 1:\n",
    "                b = x_test[1].reshape(1,-1)\n",
    "                b = preprocessing.normalize(b,axis=1)\n",
    "                x_test = np.array([b]).reshape(1,25)\n",
    "            x_test = preprocessing.normalize(x_test,axis=1)\n",
    "            if x_test_anglediff.shape[0] == 1:\n",
    "                b = x_test_anglediff[1].reshape(1,-1)\n",
    "                b = preprocessing.normalize(b,axis=1)\n",
    "                x_test_anglediff = np.array([b]).reshape(1,25)\n",
    "            x_test_anglediff = preprocessing.normalize(x_test,axis=1)\n",
    "            \n",
    "            distances_test = gen_ext.transform(x_test)\n",
    "            distances_test_anglediff = gen_ext_anglediff.transform(x_test_anglediff)\n",
    "            \n",
    "            mixed_distances = np.concatenate((distances_test,distances_test_anglediff),axis=1) \n",
    "            \n",
    "            predictions.append(lr_mixed.predict(mixed_distances))\n",
    "            proba.append(lr_mixed.predict_proba(mixed_distances))\n",
    "    return predictions,proba\n",
    "\n",
    "def results(movement_list,proba,predictions,proba_filter=False):\n",
    "    proba_info = []\n",
    "    pattern_indexes = []\n",
    "    for idx,x in enumerate(movement_list):\n",
    "        count = 0\n",
    "        indexes = []\n",
    "        for ind,i in enumerate(predictions[idx]):\n",
    "            if proba_filter:\n",
    "                if i ==0 and proba[idx][ind][0] > 0.75:\n",
    "                    count = count + 1\n",
    "                    proba_info.append({\"pattern\":x,\"first_class\":proba[idx][ind][0],\"second_class\":proba[idx][ind][1],\"chunk\":ind})\n",
    "                    indexes.append(ind)\n",
    "                else:\n",
    "                    if i ==0:\n",
    "                        count = count + 1\n",
    "                        proba_info.append({\"pattern\":x,\"first_class\":proba[idx][ind][0],\"second_class\":proba[idx][ind][1],\"chunk\":ind})\n",
    "                        indexes.append(ind)\n",
    "        print(count,\"times of pattern detection:\",x,\" at indexes \",indexes)\n",
    "        pattern_indexes.append(indexes)\n",
    "    return proba_info,pattern_indexes\n",
    "\n",
    "def fig_dir_creation(path,reset=False):\n",
    "    if not reset :\n",
    "        try:\n",
    "            os.mkdir(path)\n",
    "        except OSError:  \n",
    "            print (\"Creation of the directory %s failed\" % path)\n",
    "    else:\n",
    "        if len(glob.glob(path))==0:\n",
    "            os.mkdir(path)\n",
    "        else:\n",
    "            shutil.rmtree(path)\n",
    "            os.mkdir(path)\n",
    "            \n",
    "def find_best_patterns(proba_info):\n",
    "    a=pd.DataFrame(proba_info,columns=[\"pattern\",\"chunk\",\"first_class\",\"second_class\"])\n",
    "    a.set_index(\"chunk\",inplace=True)\n",
    "    grouped_df = a[a.groupby(\"chunk\").count()[\"pattern\"]>1].groupby(\"chunk\")\n",
    "    best_patterns = []\n",
    "    for key, item in grouped_df:\n",
    "        temp = item[grouped_df.get_group(key).loc[key][\"first_class\"]==grouped_df.get_group(key).max()[\"first_class\"]]\n",
    "        best_patterns.append([key,temp.loc[key][\"pattern\"]])\n",
    "    for x in best_patterns:\n",
    "        print(\"For chunk no:\",x[0],\"the best pattern is \",x[1])   \n",
    "    return best_patterns\n",
    "\n",
    "def plots(movement_list,\n",
    "          pattern_indexes,\n",
    "          best_patterns,\n",
    "          ship_name,\n",
    "          ship_dfs,\n",
    "          file=\"normal\",\n",
    "          scaled_down=True,\n",
    "          rdp_data=None,\n",
    "          save=False):\n",
    "    chunks = []\n",
    "    patterns = []\n",
    "    for x in best_patterns:\n",
    "        chunks.append(x[0])\n",
    "        patterns.append(x[1])\n",
    "    if save:\n",
    "        if not os.path.isdir(\"figures/\"+ship_name):\n",
    "            os.mkdir(\"figures/\"+ship_name)\n",
    "        if not os.path.isdir(\"figures/\"+ship_name+\"/\"+file):\n",
    "            os.mkdir(\"figures/\"+ship_name+\"/\"+file)\n",
    "    base_path = \"figures/\"+ship_name+\"/\"+file\n",
    "    for idx,x in enumerate(movement_list):\n",
    "        path = base_path+\"/\"+x\n",
    "        experiment = file+\"\\n\"+x\n",
    "        if save:\n",
    "            if not os.path.isdir(path):\n",
    "                os.mkdir(path)\n",
    "        for ind,i in enumerate(pattern_indexes[idx]):\n",
    "            if i in chunks:\n",
    "                if(patterns[chunks.index(i)] == x):\n",
    "                    plot_trajectory(ship_dfs[i],\n",
    "                                    train_test_options[\"split\"],\n",
    "                                    i,\n",
    "                                    ship_name,\n",
    "                                    scaled_down,\n",
    "                                    rdp_data,\n",
    "                                    experiment,\n",
    "                                    save,\n",
    "                                    path)\n",
    "            else:\n",
    "                plot_trajectory(ship_dfs[i],\n",
    "                                train_test_options[\"split\"],\n",
    "                                i,\n",
    "                                ship_name,\n",
    "                                scaled_down,\n",
    "                                rdp_data,\n",
    "                                experiment,\n",
    "                                save,\n",
    "                                path)\n",
    "\n",
    "def epsilon_selection(real_data_len,data,size,epsilon=0):\n",
    "    if real_data_len<=size:\n",
    "        return False\n",
    "    else:\n",
    "        epsilon = epsilon\n",
    "        i = 0 \n",
    "        mask_len = real_data_len\n",
    "        while mask_len!=size:\n",
    "#             print(\"i:\",i,mask_len,size,epsilon)\n",
    "            mask = rdp(data,algo=\"iter\", return_mask=True,epsilon=epsilon)\n",
    "            mask_len = mask.count(True)\n",
    "            if i >200:\n",
    "                return False\n",
    "            if mask_len < size:\n",
    "                if size - mask_len <=1:\n",
    "                    epsilon = epsilon - 0.0000000005\n",
    "                else :\n",
    "                    epsilon = epsilon - 0.000005\n",
    "            else:\n",
    "                if mask_len -size <=1:\n",
    "                    epsilon = epsilon + 0.00000002\n",
    "                else:\n",
    "                    epsilon = epsilon + 0.000002\n",
    "            i = i +1\n",
    "        return mask\n",
    "\n",
    "def print_results(movement_list,predictions,proba):\n",
    "    for id_pat,patt in enumerate(movement_list):\n",
    "        for id_pre,pred in enumerate(predictions[id_pat]):\n",
    "            print(\"for chunk {2} and pattern {0} a prediction is made with probability {1}\".format(patt,proba[id_pat][id_pre][0],id_pre)) if pred == 0 else None\n",
    "\n",
    "def plot_trajectory(data,\n",
    "                    split,\n",
    "                    chunk_no,\n",
    "                    shipname,\n",
    "                    scaled_down=True,\n",
    "                    rdp_data=None,\n",
    "                    experiment = \"\",\n",
    "                    save=False,\n",
    "                    path=\"\"):\n",
    "    df_list = []\n",
    "    data_list = []\n",
    "    real_data=data.assign(Trajectory=\"Real Trajectory\")\n",
    "    df_list.append(real_data)\n",
    "    data_list.append(real_data[\"Trajectory\"].iloc[0])\n",
    "    pallete = ['r']\n",
    "    sizes = [6]\n",
    "    dashes = [(5,5)]\n",
    "    markers = [\"^\"]\n",
    "    if scaled_down:         \n",
    "        data_scaled=scale_down(data,split).assign(Trajectory=\"Scaled Trajectory\")\n",
    "        if len(data_scaled) == split:\n",
    "            df_list.append(data_scaled)\n",
    "            data_list.append(data_scaled[\"Trajectory\"].iloc[0])\n",
    "            pallete.append('b')\n",
    "            sizes.append(3)\n",
    "            dashes.append(\"\")\n",
    "            markers.append(\"o\")\n",
    "\n",
    "    \n",
    "    if rdp_data is not None and type(rdp_data) == type(pd.DataFrame()) and len(rdp_data)>0:\n",
    "        rdp_data=data.assign(Trajectory=\"RDP Trajectory\")\n",
    "        df_list.append(rdp_data)\n",
    "        data_list.append(rdp_data[\"Trajectory\"].iloc[0])\n",
    "        pallete.append('g')\n",
    "        sizes.append(1)\n",
    "        dashes.append((5, 10))\n",
    "        markers.append(\"o\")\n",
    "\n",
    "    merged_data = pd.concat(df_list)\n",
    "\n",
    "    fig, ax= plt.subplots(1, 1, figsize=(30, 20),sharex=True)    \n",
    "    start = datetime.strftime(data[\"TIMESTAMP\"].head(1).iloc[0],'%Y-%m-%d %H:%M:%S')\n",
    "    end = datetime.strftime(data[\"TIMESTAMP\"].tail(1).iloc[0],'%Y-%m-%d %H:%M:%S')\n",
    "    title = \"Vessel:\"+shipname+\"\\nClass:\"+experiment+\"\\nCHUNK NO: \"+str(chunk_no)+\"\\n\"+start+\"----\"+end\n",
    "    ax.set_title(title,fontsize=20)\n",
    "    sns.lineplot(x='LON', y='LAT',\n",
    "                      data=merged_data,\n",
    "                      ax=ax,\n",
    "                      sort=False,\n",
    "                      hue = \"Trajectory\",\n",
    "                      palette = pallete,\n",
    "                      hue_order= data_list,\n",
    "                      size=merged_data[\"Trajectory\"],\n",
    "                      sizes = sizes,\n",
    "                      size_order = data_list,\n",
    "                      style = merged_data[\"Trajectory\"],\n",
    "                      style_order = data_list,\n",
    "                      dashes = dashes,\n",
    "                      markers = markers,\n",
    "                      legend =\"full\",\n",
    "                     )\n",
    "    lgnd = plt.legend(loc=\"lower left\", numpoints=1, fontsize=25)\n",
    "    ax.set_ylabel('Latitute', fontsize = 20.0) # Y label\n",
    "    ax.set_xlabel('Longitude ', fontsize = 20.0) # X label\n",
    "    plt.savefig(path+\"/chunk_no_\"+str(chunk_no)+\".png\",dpi=fig.dpi) if save and path!=\"\" else None\n",
    "    \n",
    "def read_ship_csv(file):\n",
    "    ship =  pd.read_csv(file)\n",
    "    shipname = ship.loc[0][\"SHIPNAME\"]\n",
    "    ship = ship [[\"TIMESTAMP\",\"LAT\",\"LON\",\"HEADING\"]]\n",
    "    ship['TIMESTAMP'] = pd.to_datetime(ship['TIMESTAMP'])  \n",
    "    ship.sort_values('TIMESTAMP',inplace=True)\n",
    "    ship=ship.reset_index(drop=True)\n",
    "    return ship,shipname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ship,shipname=read_ship_csv(\"ships/SIEM PILOT.csv\")\n",
    "# n = 500  #chunk row size\n",
    "# ship_dfs = [ship[i:i+n] for i in range(0,ship.shape[0],n)]\n",
    "# ship_data_chunked = []\n",
    "# ship_data_chunked_index = [False for i in range(0,len(ship_dfs))]  \n",
    "# for idx,x in enumerate(ship_dfs):\n",
    "#     x = scale_down(x,train_test_options[\"split\"])\n",
    "#     x = np.array(x[\"HEADING\"].values.astype(int))\n",
    "#     if len(x) == train_test_options[\"split\"]:\n",
    "#         ship_data_chunked_index[idx] = True\n",
    "#         ship_data_chunked.append(x)\n",
    "# ship_data_chunked = np.array(ship_data_chunked)\n",
    "# ship_dfs = [ship for (ship,index) in zip(ship_dfs,ship_data_chunked_index) if  index ]\n",
    "# ship ={\"shipname\":shipname,\"data\":ship_data_chunked,\"ship_dfs\":ship_dfs}\n",
    "# print(\"ERROR DIFFERENT SIZE OF DATA\") if len(ship_dfs)!=ship[\"data\"].shape[0] else None\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# print(\"\\nVessel:\"+ship[\"shipname\"]+\"\\nNormal Setting Results\\n\")\n",
    "    \n",
    "# predictions,proba=experiments(ship=ship,movement_list=movement_list)\n",
    "# proba_info,pattern_indexes = results(movement_list=movement_list,\n",
    "#                                      proba=proba,\n",
    "#                                      predictions=predictions,\n",
    "#                                      proba_filter=True)\n",
    "# best_patterns=find_best_patterns(proba_info)\n",
    "# plots(movement_list=movement_list,\n",
    "#       pattern_indexes=pattern_indexes,\n",
    "#       best_patterns=best_patterns,\n",
    "#       ship_name=ship[\"shipname\"],\n",
    "#       ship_dfs=ship[\"ship_dfs\"],\n",
    "#       save=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_dir_creation(\"figures\",True)\n",
    "if len(glob.glob('ships')) != 0 :\n",
    "    files = glob.glob(\"ships/*.csv\")\n",
    "    all_ships = [] \n",
    "    for file in files:\n",
    "        ship,shipname=read_ship_csv(file)\n",
    "        n = 500  #chunk row size\n",
    "        ship_dfs = [ship[i:i+n] for i in range(0,ship.shape[0],n)]\n",
    "        ship_data_chunked = []\n",
    "        ship_data_chunked_index = [False for i in range(0,len(ship_dfs))]  \n",
    "        for idx,x in enumerate(ship_dfs):\n",
    "            x = scale_down(x,train_test_options[\"split\"])\n",
    "            x = np.array(x[\"HEADING\"].values.astype(int))\n",
    "            if len(x) == train_test_options[\"split\"]:\n",
    "                ship_data_chunked_index[idx] = True\n",
    "                ship_data_chunked.append(x)\n",
    "        ship_data_chunked = np.array(ship_data_chunked)\n",
    "        ship_dfs = [ship for (ship,index) in zip(ship_dfs,ship_data_chunked_index) if  index ]\n",
    "        ship ={\"shipname\":shipname,\"data\":ship_data_chunked,\"ship_dfs\":ship_dfs}\n",
    "        print(\"ERROR DIFFERENT SIZE OF DATA\") if len(ship_dfs)!=ship[\"data\"].shape[0] else None\n",
    "        all_ships.append(ship)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ship in all_ships:\n",
    "    print(\"\\nVessel:\"+ship[\"shipname\"]+\"\\nNormal Setting Results\\n\")\n",
    "    \n",
    "    predictions,proba=experiments(ship=ship,movement_list=movement_list)\n",
    "    proba_info,pattern_indexes = results(movement_list=movement_list,\n",
    "                                         proba=proba,\n",
    "                                         predictions=predictions,\n",
    "                                         proba_filter=True)\n",
    "    best_patterns=find_best_patterns(proba_info)\n",
    "    plots(movement_list=movement_list,\n",
    "          pattern_indexes=pattern_indexes,\n",
    "          best_patterns=best_patterns,\n",
    "          ship_name=ship[\"shipname\"],\n",
    "          ship_dfs=ship[\"ship_dfs\"],\n",
    "          save=True)\n",
    "    \n",
    "    print(\"\\nVessel:\"+ship[\"shipname\"]+\"\\nBearing Rate Setting Results\\n\")\n",
    "\n",
    "    \n",
    "    predictions_agnlediff,proba_anglediff=experiments(ship=ship,\n",
    "                                                      movement_list=movement_list,\n",
    "                                                      type_exp=\"anglediff\")\n",
    "    proba_info_anglediff,pattern_indexes_anglediff = results(movement_list=movement_list,\n",
    "                                                             proba=proba_anglediff,\n",
    "                                                             predictions=predictions_agnlediff,\n",
    "                                                             proba_filter=True)\n",
    "    best_patterns_anglediff=find_best_patterns(proba_info_anglediff)\n",
    "    plots(movement_list=movement_list,\n",
    "          pattern_indexes=pattern_indexes_anglediff,\n",
    "          best_patterns=best_patterns_anglediff,\n",
    "          ship_name=ship[\"shipname\"],\n",
    "          ship_dfs=ship[\"ship_dfs\"],\n",
    "          file=\"anglediff\",\n",
    "          save=True)\n",
    "\n",
    "    print(\"\\nVessel:\"+ship[\"shipname\"]+\"\\nMixed Models Setting Results\\n\")\n",
    "\n",
    "    predictions_mixed,proba_mixed=experiments(ship=ship,\n",
    "                                              movement_list=movement_list,\n",
    "                                              mixed_models=True)\n",
    "    proba_info_mixed,pattern_indexes_mixed = results(movement_list=movement_list,\n",
    "                                                     proba=proba_anglediff,\n",
    "                                                     predictions=predictions_agnlediff,\n",
    "                                                     proba_filter=True)\n",
    "    best_patterns_mixed=find_best_patterns(proba_info_mixed)\n",
    "    plots(movement_list=movement_list,\n",
    "          pattern_indexes=pattern_indexes_mixed,\n",
    "          best_patterns=best_patterns_mixed,\n",
    "          ship_name=ship[\"shipname\"],\n",
    "          ship_dfs=ship[\"ship_dfs\"],\n",
    "          file=\"mixed_models\",\n",
    "          save=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
