{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tools.utils import scale_down\n",
    "from tools.utils import standardize_data, print_genetic_param, print_settings, set_movements, angle_diff, get_distance\n",
    "from statistics import mean\n",
    "from random import choice\n",
    "train_test_options = {\"split\": 25}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def check_sampling(x,sampling,data_index):\n",
    "#     i = 1\n",
    "#     sampling_acc = True\n",
    "#     down_limit = (x[1]-sampling)\n",
    "#     up_limit = (x[1]+sampling)\n",
    "#     while i<=sampling:\n",
    "#         if 0 <=down_limit and up_limit < len(data_index):\n",
    "#             if (data_index[x[1]-i] == True) or (data_index[x[1]+i] == True):\n",
    "#                 sampling_acc = False\n",
    "#                 break\n",
    "#         i = i + 1      \n",
    "#     return sampling_acc\n",
    "\n",
    "\n",
    "# def dist_and_bearing_diff(data):\n",
    "#     all_distances = []\n",
    "#     bearing_diff = []\n",
    "#     data_size=len(data)\n",
    "#     i = 0\n",
    "#     while i<data_size:\n",
    "#         if i + 1 >=data_size:\n",
    "#             break\n",
    "#         bearing_1, bearing_2 = data[\"HEADING\"].iloc[i], data[\"HEADING\"].iloc[i+1] \n",
    "#         bearing_diff.append([abs(bearing_2 - bearing_1),i])\n",
    "#         lat_1, lon_1, lat_2, lon_2 = data[\"LAT\"].iloc[i], data[\"LON\"].iloc[i], data[\"LAT\"].iloc[i+1], data[\"LON\"].iloc[i+1]\n",
    "#         all_distances.append(get_distance(lat_1,lon_1,lat_2,lon_2))\n",
    "#         i = i +1\n",
    "#     return bearing_diff, all_distances \n",
    "\n",
    "\n",
    "# def fitting_indexes(arr,new_size):\n",
    "#     i = 0\n",
    "#     r_arr = []\n",
    "#     while i <len(arr):\n",
    "#         if i+1 >=len(arr):\n",
    "#             break\n",
    "#         r_arr.append(arr[i+1]-arr[i])\n",
    "#         i = i + 1\n",
    "#     if len(r_arr) <= 0:\n",
    "#         return arr\n",
    "#     mean_space = mean(r_arr)\n",
    "#     i = 0 \n",
    "#     while i<len(arr)<new_size:\n",
    "#         if i+1 >=len(arr):\n",
    "#             break\n",
    "#         diff = arr[i+1]-arr[i]\n",
    "#         if mean_space <= diff:\n",
    "#             step = int(diff/2)\n",
    "#             new_index = arr[i] + step\n",
    "#             arr.insert(i+1,new_index)\n",
    "#             i = i + 1\n",
    "#         i = i + 1\n",
    "#     return arr\n",
    "\n",
    "# def scalling_down_windowed(data,n_sample,turn_sensitivity=30):\n",
    "#     if len(data) <= n_sample :\n",
    "#         return data\n",
    "#     size_correction = int(len(data) / n_sample) * n_sample\n",
    "#     data=data[:size_correction]\n",
    "#     data_size=len(data)\n",
    "#     data_index = [False for i in range(data_size)]\n",
    "#     sampling = int((int(data_size/n_sample) * 0.25))\n",
    "#     labels = data.columns\n",
    "    \n",
    "#     temp_idx = [] \n",
    "#     final_data = pd.DataFrame(data,columns=labels)\n",
    "#     final_data.reset_index(drop=True)\n",
    "    \n",
    "#     #find the number of bearing differance above the turn sensitivity\n",
    "#     bearing_diff , all_distances = dist_and_bearing_diff(final_data)\n",
    "#     mean_dist = mean(all_distances)\n",
    "#     for idx, x in enumerate(bearing_diff):\n",
    "#         sampling_acc = check_sampling(x,sampling,data_index)\n",
    "#         if (x[0] > turn_sensitivity) and (all_distances[idx] > mean_dist/2) and sampling_acc:\n",
    "#             data_index[x[1]] = True\n",
    "#             temp_idx.append(x[1])\n",
    "#     while n_sample > len(temp_idx) and len(temp_idx) > 1 :\n",
    "#         fitting_indexes(temp_idx,n_sample)\n",
    "\n",
    "#     for x in temp_idx:\n",
    "#         data_index[x]=True\n",
    "        \n",
    "#     return final_data[data_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if len(glob.glob('ships')) != 0 :\n",
    "#     files = glob.glob(\"ships/*.csv\")\n",
    "#     all_ships = [] \n",
    "#     for file in files:\n",
    "#         ship =  pd.read_csv(file)\n",
    "#         shipname = ship.loc[0][\"SHIPNAME\"]\n",
    "#         ship = ship [[\"TIMESTAMP\",\"LAT\",\"LON\",\"HEADING\"]]\n",
    "#         ship['TIMESTAMP'] = pd.to_datetime(ship['TIMESTAMP'])  \n",
    "#         ship.sort_values('TIMESTAMP',inplace=True)\n",
    "#         ship=ship.reset_index(drop=True)\n",
    "#         n = 500  #chunk row size\n",
    "#         ship_dfs = [ship[i:i+n] for i in range(0,ship.shape[0],n)]\n",
    "#         ship_data_chunked = []\n",
    "#         for x in ship_dfs:\n",
    "#             x = scale_down(x,train_test_options[\"split\"])\n",
    "#             x = np.array(x[\"HEADING\"].values.astype(int))\n",
    "#             ship_data_chunked.append(x)\n",
    "#         ship_data_chunked = np.array(ship_data_chunked)\n",
    "#         ship ={\"shipname\":shipname,\"data\":ship_data_chunked}\n",
    "#     all_ships.append(ship)\n",
    "# #     print(all_ships)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(36, 25)\n"
     ]
    }
   ],
   "source": [
    "ship =  pd.read_csv(\"ships/SIEM PILOT.csv\")\n",
    "shipname = ship.loc[0][\"SHIPNAME\"]\n",
    "ship = ship [[\"TIMESTAMP\",\"LAT\",\"LON\",\"HEADING\"]]\n",
    "ship['TIMESTAMP'] = pd.to_datetime(ship['TIMESTAMP'])  \n",
    "ship.sort_values('TIMESTAMP',inplace=True)\n",
    "ship=ship.reset_index(drop=True)\n",
    "# ship = scale_down(ship[0:500],train_test_options[\"split\"])\n",
    "n = 500  #chunk row size\n",
    "ship_dfs = [ship[i:i+n] for i in range(0,ship.shape[0],n)]\n",
    "ship_data_chunked = []\n",
    "for idx,x in enumerate(ship_dfs):\n",
    "    x = scale_down(x,train_test_options[\"split\"])\n",
    "    x = np.array(x[\"HEADING\"].values.astype(int))\n",
    "    if len(x) == train_test_options[\"split\"]:\n",
    "        ship_data_chunked.append(x)\n",
    "ship_data_chunked = np.array(ship_data_chunked)\n",
    "ship ={\"shipname\":shipname,\"data\":ship_data_chunked}\n",
    "print(ship[\"data\"].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generation(movement):\n",
    "    from tools.data_extraction import DataExtractor\n",
    "    from tools.trajectory_generator import TrajectoryGenerator\n",
    "    from tools.utils import standardize_data, print_genetic_param, print_settings, set_movements, angle_diff,scale_down, get_distance\n",
    "    tr_gen_options = {\"samples\": 25,\n",
    "                  \"freq\": 3,\n",
    "                  \"reset_data\": True}\n",
    "    dt_gen_options = {\"n_test\": 150}\n",
    "\n",
    "    train_test_options = {\"split\": 25}\n",
    "\n",
    "    df_csv_options = {\"ts_class\": \"Bearing\"}\n",
    "    settings = {\"trajectory_generator_options\": tr_gen_options,\n",
    "            \"data_generation_options\": dt_gen_options,\n",
    "            \"train_test_options\":train_test_options,\n",
    "            \"define_csvs_option\": df_csv_options}\n",
    "    tr_gen_options = settings[\"trajectory_generator_options\"]\n",
    "    dt_gen_options = settings[\"data_generation_options\"]\n",
    "    df_csvs_options = settings[\"define_csvs_option\"]\n",
    "    train_test_options = settings[\"train_test_options\"]\n",
    "    first_movement = [movement]\n",
    "    second_movement = ['random']\n",
    "    movements = {'first_movement': first_movement,'second_movement': second_movement}\n",
    "    set_movements(movements)\n",
    "    tr_gen = TrajectoryGenerator(**tr_gen_options)\n",
    "    tr_gen.data_generation(**dt_gen_options)\n",
    "    dex = DataExtractor()\n",
    "    train_df, test_df = dex.train_test_dataframes(**train_test_options)\n",
    "    dex.define_csv(**df_csvs_options)\n",
    "    x_train, y_train, x_test, y_test = dex.load_datasets()\n",
    "    return x_train, y_train, x_test, y_test\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-05-24 15:06:02.565 | INFO     | tools.trajectory_generator:data_generation:623 - \n",
      " Starting the generator with attributes: \n",
      "Original latitude: 37.295493\n",
      "Original longitude: 23.824322\n",
      "Initial bearing: 90\n",
      "Initial speed: 10\n",
      "Number of samples: 25\n",
      "Starting time of measurements: 2015-02-01 12:00:00\n",
      "With initial frequency of collected data: 3 min\n",
      "and hard reset of data: True\n",
      "2019-05-24 15:06:02.569 | INFO     | tools.trajectory_generator:data_generation:626 - Create directory 'generator_data' \n",
      "2019-05-24 15:06:02.570 | INFO     | tools.trajectory_generator:data_generation:632 - now creating data for movement: spiral_movement_right\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment for pattern spiral_movement_right\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-05-24 15:06:36.376 | INFO     | tools.trajectory_generator:data_generation:637 - now creating data for movement: random\n",
      "2019-05-24 15:07:07.629 | SUCCESS  | tools.trajectory_generator:data_generation:670 - Done with generator\n",
      "2019-05-24 15:07:07.631 | INFO     | tools.data_extraction:read_datasets:22 - Reading the data files\n",
      "2019-05-24 15:07:08.374 | SUCCESS  | tools.data_extraction:read_datasets:51 - Done reading files\n",
      "2019-05-24 15:07:09.393 | INFO     | tools.data_extraction:define_csv:76 - Creating x_train.csv--y_train.csv and x_test.csv--y_test.csv \n",
      "2019-05-24 15:07:09.432 | SUCCESS  | tools.data_extraction:define_csv:101 - Done with train.csv\n",
      "2019-05-24 15:07:09.445 | INFO     | tools.data_extraction:define_csv:122 - Done with test.csv\n",
      "2019-05-24 15:07:09.446 | INFO     | tools.data_extraction:load_datasets:129 - Loading the csv files to the appropriate train and test arrays(nparrays)\n",
      "2019-05-24 15:07:09.467 | SUCCESS  | tools.data_extraction:load_datasets:134 - Done\n",
      "/home/kapadais/anaconda3/envs/gendis_test/lib/python3.6/site-packages/sklearn/utils/validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "/home/kapadais/anaconda3/envs/gendis_test/lib/python3.6/site-packages/sklearn/utils/validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "2019-05-24 15:07:15.383 | INFO     | tools.trajectory_generator:data_generation:623 - \n",
      " Starting the generator with attributes: \n",
      "Original latitude: 37.295493\n",
      "Original longitude: 23.824322\n",
      "Initial bearing: 90\n",
      "Initial speed: 10\n",
      "Number of samples: 25\n",
      "Starting time of measurements: 2015-02-01 12:00:00\n",
      "With initial frequency of collected data: 3 min\n",
      "and hard reset of data: True\n",
      "2019-05-24 15:07:15.385 | INFO     | tools.trajectory_generator:data_generation:626 - Create directory 'generator_data' \n",
      "2019-05-24 15:07:15.388 | INFO     | tools.trajectory_generator:data_generation:632 - now creating data for movement: step_up_right\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment for pattern step_up_right\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-05-24 15:07:27.499 | INFO     | tools.trajectory_generator:data_generation:637 - now creating data for movement: random\n",
      "2019-05-24 15:07:58.539 | SUCCESS  | tools.trajectory_generator:data_generation:670 - Done with generator\n",
      "2019-05-24 15:07:58.541 | INFO     | tools.data_extraction:read_datasets:22 - Reading the data files\n",
      "2019-05-24 15:07:59.409 | SUCCESS  | tools.data_extraction:read_datasets:51 - Done reading files\n",
      "2019-05-24 15:08:00.460 | INFO     | tools.data_extraction:define_csv:76 - Creating x_train.csv--y_train.csv and x_test.csv--y_test.csv \n",
      "2019-05-24 15:08:00.491 | SUCCESS  | tools.data_extraction:define_csv:101 - Done with train.csv\n",
      "2019-05-24 15:08:00.499 | INFO     | tools.data_extraction:define_csv:122 - Done with test.csv\n",
      "2019-05-24 15:08:00.504 | INFO     | tools.data_extraction:load_datasets:129 - Loading the csv files to the appropriate train and test arrays(nparrays)\n",
      "2019-05-24 15:08:00.521 | SUCCESS  | tools.data_extraction:load_datasets:134 - Done\n",
      "/home/kapadais/anaconda3/envs/gendis_test/lib/python3.6/site-packages/sklearn/utils/validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "/home/kapadais/anaconda3/envs/gendis_test/lib/python3.6/site-packages/sklearn/utils/validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "2019-05-24 15:08:08.120 | INFO     | tools.trajectory_generator:data_generation:623 - \n",
      " Starting the generator with attributes: \n",
      "Original latitude: 37.295493\n",
      "Original longitude: 23.824322\n",
      "Initial bearing: 90\n",
      "Initial speed: 10\n",
      "Number of samples: 25\n",
      "Starting time of measurements: 2015-02-01 12:00:00\n",
      "With initial frequency of collected data: 3 min\n",
      "and hard reset of data: True\n",
      "2019-05-24 15:08:08.122 | INFO     | tools.trajectory_generator:data_generation:626 - Create directory 'generator_data' \n",
      "2019-05-24 15:08:08.127 | INFO     | tools.trajectory_generator:data_generation:632 - now creating data for movement: spiral_movement_right\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment for pattern spiral_movement_right\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-05-24 15:08:41.260 | INFO     | tools.trajectory_generator:data_generation:637 - now creating data for movement: random\n"
     ]
    }
   ],
   "source": [
    "movement_list = [\"spiral_movement_right\",\"step_up_right\",\n",
    "                 \"spiral_movement_right\",\"spiral_movement_left\",\n",
    "                 \"expanding_square_right\",\"expanding_square_left\",\n",
    "                 \"creeping_line_left\",\"creeping_line_right\",\n",
    "                 \"sector_pattern_left\",\"sector_pattern_right\"]\n",
    "from sklearn.externals import joblib\n",
    "from gendis.genetic import GeneticExtractor\n",
    "predictions = [] \n",
    "for x in movement_list:\n",
    "    print(\"Experiment for pattern\",x)\n",
    "    with open(\"models/\"+x+\".pkl\", 'rb') as pickle_file:\n",
    "        gen_ext = joblib.load( pickle_file)\n",
    "        x_train, y_train, x_test, y_test = data_generation(x)\n",
    "        x_test = ship[\"data\"]\n",
    "        x_train, x_test = standardize_data(x_train, x_test)\n",
    "        distances_train = gen_ext.transform(x_train)\n",
    "        lr = LogisticRegression()\n",
    "        lr.fit(distances_train, y_train)\n",
    "        distances_test = gen_ext.transform(x_test)\n",
    "        predictions.append(lr.predict(distances_test))\n",
    "        pickle_file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import seaborn as sns\n",
    "for idx,x in enumerate(movement_list):\n",
    "    count = 0\n",
    "    indexes = []\n",
    "    for ind,i in enumerate(predictions[idx]):\n",
    "        if i ==0:\n",
    "            count = count + 1\n",
    "            indexes.append(ind)\n",
    "            fig, ax = plt.subplots()\n",
    "            fig.set_size_inches(30, 20)\n",
    "            ax.set_ylabel('Latitute', fontsize = 20.0) # Y label\n",
    "            ax.set_xlabel('Longitude ', fontsize = 20.0) # X label\n",
    "            start = datetime.strftime(ship_dfs[ind][\"TIMESTAMP\"].head(1).iloc[0],'%Y-%m-%d %H:%M:%S')\n",
    "            end = datetime.strftime(ship_dfs[ind][\"TIMESTAMP\"].tail(1).iloc[0],'%Y-%m-%d %H:%M:%S')\n",
    "            title = \"Experiment:\"+x+\"\\nCHUNK NO: \"+str(ind)+\"\\n\"+start+\"----\"+end\n",
    "            ax.set_title(title,fontsize = 20.0)\n",
    "            data=ship_dfs[ind]\n",
    "#             data=scale_down(ship_dfs[ind],25)\n",
    "            sns.lineplot(x='LON', y='LAT', data=data, ax=ax,sort=False,marker=\"o\")\n",
    "    print(count,\"times of pattern detection:\",x,\" at indexes \",indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1,\n",
       "        0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1,\n",
       "        0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1]),\n",
       " array([0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1,\n",
       "        0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1,\n",
       "        0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1]),\n",
       " array([0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1,\n",
       "        0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1,\n",
       "        0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1]),\n",
       " array([0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1,\n",
       "        0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1,\n",
       "        0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0]),\n",
       " array([0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1,\n",
       "        0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1,\n",
       "        0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1]),\n",
       " array([0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1,\n",
       "        0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1,\n",
       "        0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1]),\n",
       " array([1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0,\n",
       "        0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1,\n",
       "        0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1]),\n",
       " array([0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1,\n",
       "        0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1,\n",
       "        0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1]),\n",
       " array([0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1,\n",
       "        0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1,\n",
       "        0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1]),\n",
       " array([0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1,\n",
       "        0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0,\n",
       "        0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0])]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
