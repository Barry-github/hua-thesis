{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "results_1=np.array([[0.825, 0.85, 0.825, 0.975, 0.75, 0.9333333333333333, 0.8, 0.8333333333333334, 0.9833333333333333, 1.0, 0.85, 0.9333333333333333, 1.0, 1.0, 0.95, 0.9833333333333333], [0.725, 0.825, 0.9, 0.9, 0.95, 0.85, 0.7666666666666667, 0.95, 1.0, 0.9666666666666667, 1.0, 0.9166666666666666, 1.0, 0.9833333333333333, 0.9333333333333333, 1.0], [0.925, 0.775, 0.95, 0.95, 0.9166666666666666, 0.8833333333333333, 0.8166666666666667, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9666666666666667, 0.95, 1.0, 1.0, 0.9666666666666667, 0.95], [0.8, 0.95, 0.975, 0.55, 0.8833333333333333, 0.95, 0.9166666666666666, 0.9666666666666667, 1.0, 0.9833333333333333, 0.9333333333333333, 0.8833333333333333, 1.0, 1.0, 0.9666666666666667, 0.9833333333333333], [0.65, 0.925, 0.975, 0.925, 0.8666666666666667, 0.9333333333333333, 0.9, 0.95, 0.9833333333333333, 0.9166666666666666, 0.9666666666666667, 0.9, 1.0, 1.0, 0.95, 0.95], [0.775, 0.925, 0.75, 0.75, 0.8666666666666667, 0.6166666666666667, 0.9666666666666667, 0.7833333333333333, 1.0, 0.9166666666666666, 1.0, 0.9333333333333333, 1.0, 1.0, 0.9666666666666667, 0.8666666666666667], [0.775, 0.925, 0.725, 0.9, 0.9, 0.7833333333333333, 0.9166666666666666, 0.9833333333333333, 1.0, 0.9666666666666667, 0.9833333333333333, 0.7333333333333333, 0.9666666666666667, 1.0, 0.95, 1.0], [0.925, 0.925, 0.85, 0.825, 0.9166666666666666, 0.8333333333333334, 0.8166666666666667, 0.9333333333333333, 1.0, 0.9833333333333333, 0.9833333333333333, 0.9666666666666667, 1.0, 1.0, 0.9833333333333333, 0.9833333333333333], [0.925, 0.825, 0.85, 0.825, 0.8, 0.9333333333333333, 0.7166666666666667, 0.8666666666666667, 0.9833333333333333, 1.0, 0.95, 0.9166666666666666, 1.0, 0.9833333333333333, 0.9666666666666667, 0.95], [0.9, 0.9, 0.725, 0.85, 0.8166666666666667, 0.9166666666666666, 0.9666666666666667, 0.8833333333333333, 1.0, 0.9333333333333333, 1.0, 0.9833333333333333, 1.0, 1.0, 1.0, 0.9166666666666666], [0.8, 0.875, 0.9, 0.9, 0.7166666666666667, 0.9166666666666666, 0.9166666666666666, 0.8833333333333333, 0.9333333333333333, 0.9833333333333333, 0.9833333333333333, 0.9166666666666666, 1.0, 1.0, 0.9333333333333333, 0.9666666666666667], [0.875, 1.0, 0.8, 0.9, 0.9166666666666666, 0.7833333333333333, 0.95, 0.9833333333333333, 0.9333333333333333, 0.9833333333333333, 0.9833333333333333, 0.9333333333333333, 0.9833333333333333, 1.0, 0.9666666666666667, 0.9333333333333333], [0.95, 0.9, 0.875, 0.8, 0.7833333333333333, 0.95, 0.85, 0.95, 1.0, 0.9833333333333333, 0.9833333333333333, 0.8666666666666667, 1.0, 0.9666666666666667, 0.9833333333333333, 0.9833333333333333], [0.9, 0.925, 0.925, 0.925, 0.95, 0.8166666666666667, 0.9333333333333333, 0.8333333333333334, 1.0, 1.0, 0.9833333333333333, 0.9666666666666667, 1.0, 1.0, 0.8833333333333333, 0.9666666666666667], [0.85, 0.875, 0.725, 0.775, 0.9333333333333333, 0.8666666666666667, 0.9, 0.9833333333333333, 1.0, 1.0, 1.0, 0.8666666666666667, 1.0, 1.0, 1.0, 0.95], [0.725, 0.875, 0.9, 0.925, 0.85, 0.8833333333333333, 0.7833333333333333, 0.8333333333333334, 1.0, 0.9833333333333333, 0.95, 0.8333333333333334, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.95], [0.8, 0.8, 0.975, 0.95, 0.9166666666666666, 0.9166666666666666, 0.95, 0.8166666666666667, 1.0, 0.9833333333333333, 0.95, 0.9166666666666666, 1.0, 1.0, 0.9166666666666666, 0.9666666666666667], [0.875, 0.775, 0.85, 0.875, 0.75, 0.9333333333333333, 0.8666666666666667, 0.7666666666666667, 1.0, 1.0, 0.7666666666666667, 0.9833333333333333, 1.0, 1.0, 0.9333333333333333, 0.9333333333333333], [0.8, 0.8, 0.8, 0.925, 0.9166666666666666, 0.85, 0.7833333333333333, 0.9333333333333333, 1.0, 0.8666666666666667, 0.8166666666666667, 0.9166666666666666, 1.0, 1.0, 0.9666666666666667, 0.9333333333333333], [0.55, 0.9, 0.95, 0.925, 0.7833333333333333, 0.9333333333333333, 0.9, 0.9333333333333333, 1.0, 1.0, 1.0, 0.8666666666666667, 1.0, 0.9833333333333333, 0.9666666666666667, 0.95], [0.875, 0.9, 0.8, 0.9, 0.9, 0.95, 0.6666666666666666, 0.9, 1.0, 0.9833333333333333, 0.9666666666666667, 0.9666666666666667, 1.0, 0.9833333333333333, 1.0, 0.9333333333333333], [0.85, 0.525, 0.975, 0.95, 0.8333333333333334, 0.7666666666666667, 0.8666666666666667, 0.9833333333333333, 1.0, 0.9833333333333333, 0.7, 0.9666666666666667, 1.0, 1.0, 0.9833333333333333, 0.9166666666666666], [0.9, 0.975, 0.825, 0.75, 0.8833333333333333, 0.8833333333333333, 0.8666666666666667, 0.95, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 0.9833333333333333, 0.95], [0.925, 0.9, 0.925, 0.975, 0.7666666666666667, 0.8666666666666667, 0.9, 0.9333333333333333, 1.0, 1.0, 0.9166666666666666, 0.8, 1.0, 1.0, 0.9833333333333333, 1.0], [1.0, 0.875, 0.75, 0.7, 0.9166666666666666, 0.8, 0.8833333333333333, 0.9166666666666666, 1.0, 0.9166666666666666, 0.9833333333333333, 0.9333333333333333, 1.0, 1.0, 0.95, 0.9166666666666666], [0.825, 0.975, 0.975, 0.9, 0.6666666666666666, 0.8666666666666667, 0.8666666666666667, 0.9333333333333333, 0.9833333333333333, 0.95, 0.8333333333333334, 0.8666666666666667, 1.0, 1.0, 0.85, 0.9666666666666667], [0.725, 0.475, 0.8, 0.925, 0.8666666666666667, 0.9166666666666666, 0.9, 0.9666666666666667, 0.9833333333333333, 0.9, 0.95, 0.6833333333333333, 0.95, 1.0, 0.9333333333333333, 0.9166666666666666], [0.95, 0.975, 0.9, 0.8, 0.7833333333333333, 0.9333333333333333, 0.8666666666666667, 0.9166666666666666, 1.0, 0.95, 0.9666666666666667, 0.9166666666666666, 1.0, 1.0, 0.9833333333333333, 0.9], [0.95, 0.75, 0.95, 0.725, 0.9, 0.95, 0.8833333333333333, 0.9333333333333333, 0.9833333333333333, 0.95, 0.9333333333333333, 0.9666666666666667, 1.0, 1.0, 0.9833333333333333, 0.95], [0.75, 0.925, 0.6, 0.85, 0.8833333333333333, 0.9166666666666666, 0.8666666666666667, 0.8333333333333334, 0.9833333333333333, 0.95, 0.9333333333333333, 0.9, 1.0, 1.0, 1.0, 0.9833333333333333], [0.675, 0.925, 0.925, 0.95, 0.9, 0.8833333333333333, 0.8833333333333333, 0.9333333333333333, 1.0, 1.0, 0.9833333333333333, 0.9, 0.9833333333333333, 1.0, 0.95, 1.0], [0.9, 0.95, 0.75, 0.85, 0.9166666666666666, 0.9666666666666667, 0.9333333333333333, 0.8666666666666667, 1.0, 0.9666666666666667, 0.9833333333333333, 0.85, 1.0, 1.0, 1.0, 0.9], [0.575, 0.85, 0.9, 0.7, 0.85, 0.95, 0.9166666666666666, 0.8833333333333333, 0.9833333333333333, 1.0, 0.9833333333333333, 0.9666666666666667, 1.0, 0.9833333333333333, 0.9833333333333333, 1.0], [0.95, 0.85, 0.925, 0.975, 0.8333333333333334, 0.9333333333333333, 0.9, 0.8666666666666667, 1.0, 1.0, 0.6, 0.9833333333333333, 1.0, 1.0, 0.9666666666666667, 1.0], [0.85, 0.825, 0.75, 0.975, 0.95, 0.9666666666666667, 0.9666666666666667, 0.8333333333333334, 1.0, 0.9666666666666667, 0.8166666666666667, 0.9666666666666667, 0.9833333333333333, 0.9833333333333333, 0.9666666666666667, 0.9166666666666666], [0.8, 0.95, 0.825, 0.85, 0.7833333333333333, 0.85, 0.9333333333333333, 0.95, 1.0, 1.0, 1.0, 0.9666666666666667, 1.0, 1.0, 0.9666666666666667, 1.0], [0.675, 0.925, 0.875, 0.925, 0.9666666666666667, 0.9, 0.9666666666666667, 0.9333333333333333, 1.0, 1.0, 0.9833333333333333, 0.9, 1.0, 1.0, 0.9666666666666667, 0.9666666666666667], [0.925, 0.7, 1.0, 0.85, 0.8833333333333333, 0.9, 0.9333333333333333, 0.9666666666666667, 0.9666666666666667, 1.0, 0.8833333333333333, 0.9166666666666666, 1.0, 0.9833333333333333, 0.9833333333333333, 0.9333333333333333], [0.675, 0.8, 0.9, 0.775, 0.8333333333333334, 0.75, 0.9833333333333333, 0.8833333333333333, 1.0, 0.9666666666666667, 0.8333333333333334, 0.7833333333333333, 1.0, 0.9833333333333333, 1.0, 0.8833333333333333], [0.85, 0.9, 0.9, 0.95, 0.9666666666666667, 0.95, 0.9, 0.9166666666666666, 1.0, 1.0, 0.9833333333333333, 0.9333333333333333, 1.0, 0.9833333333333333, 0.9333333333333333, 0.9833333333333333], [0.825, 0.95, 0.675, 0.95, 0.8, 0.8833333333333333, 0.9333333333333333, 0.8666666666666667, 0.9666666666666667, 0.9833333333333333, 0.9333333333333333, 0.95, 1.0, 0.9666666666666667, 1.0, 0.9666666666666667], [0.8, 0.925, 0.775, 0.825, 0.9, 0.8833333333333333, 0.9666666666666667, 0.9333333333333333, 1.0, 1.0, 0.7666666666666667, 0.9166666666666666, 1.0, 1.0, 0.9833333333333333, 0.9666666666666667], [0.85, 0.8, 0.775, 0.825, 0.7666666666666667, 0.9, 0.9333333333333333, 0.9333333333333333, 1.0, 1.0, 0.6166666666666667, 0.9, 1.0, 0.9666666666666667, 1.0, 0.9333333333333333], [0.975, 0.725, 0.9, 0.85, 0.8833333333333333, 0.9166666666666666, 0.9666666666666667, 0.9, 1.0, 0.9833333333333333, 0.9333333333333333, 0.9833333333333333, 1.0, 0.9166666666666666, 0.9666666666666667, 1.0], [0.8, 0.825, 0.95, 0.85, 0.9, 0.8833333333333333, 0.8333333333333334, 0.95, 1.0, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9666666666666667, 0.9666666666666667, 0.7833333333333333, 0.9833333333333333], [0.95, 0.775, 0.95, 0.825, 0.8833333333333333, 0.85, 0.9, 0.9666666666666667, 1.0, 0.9666666666666667, 0.9, 0.9666666666666667, 1.0, 1.0, 0.95, 0.95], [0.825, 0.9, 0.9, 0.775, 0.9333333333333333, 0.9333333333333333, 0.9833333333333333, 0.8166666666666667, 1.0, 0.9333333333333333, 0.9666666666666667, 1.0, 1.0, 1.0, 0.9666666666666667, 0.9666666666666667], [0.75, 0.75, 0.75, 0.9, 0.9333333333333333, 0.8666666666666667, 0.7333333333333333, 0.8666666666666667, 1.0, 0.95, 0.95, 0.9333333333333333, 1.0, 1.0, 0.9833333333333333, 0.95], [0.85, 0.675, 0.75, 0.925, 0.9333333333333333, 0.8666666666666667, 0.9666666666666667, 0.9166666666666666, 1.0, 0.9833333333333333, 0.7333333333333333, 0.9666666666666667, 1.0, 1.0, 1.0, 0.95], [0.975, 0.925, 0.9, 0.825, 0.85, 0.95, 0.9666666666666667, 1.0, 1.0, 0.9333333333333333, 0.85, 0.9666666666666667, 0.9833333333333333, 1.0, 0.9666666666666667, 0.9833333333333333]])\n",
    "results_2 =np.array([[0.725, 0.775, 0.925, 0.975, 0.7833333333333333, 0.85, 0.9333333333333333, 0.9666666666666667, 0.9833333333333333, 1.0, 0.95, 0.9833333333333333, 0.9833333333333333, 1.0, 0.9333333333333333, 0.8833333333333333], [0.925, 0.675, 0.9, 0.95, 0.8333333333333334, 0.8166666666666667, 0.8833333333333333, 0.8833333333333333, 1.0, 1.0, 0.8166666666666667, 0.7833333333333333, 0.9666666666666667, 1.0, 0.9666666666666667, 0.9166666666666666], [0.9, 0.775, 0.9, 0.85, 0.65, 0.8, 0.9166666666666666, 0.7666666666666667, 0.95, 1.0, 0.9833333333333333, 0.9333333333333333, 1.0, 0.9833333333333333, 0.9333333333333333, 0.95], [0.75, 0.825, 0.65, 0.975, 0.9, 0.8, 0.9166666666666666, 0.8666666666666667, 1.0, 1.0, 0.8, 0.8333333333333334, 1.0, 1.0, 0.9666666666666667, 0.9833333333333333], [0.925, 0.925, 0.9, 0.9, 0.8666666666666667, 0.7833333333333333, 0.8666666666666667, 0.9666666666666667, 1.0, 0.9833333333333333, 0.9833333333333333, 1.0, 1.0, 1.0, 0.9666666666666667, 1.0], [0.825, 0.825, 0.85, 0.825, 0.9666666666666667, 0.8666666666666667, 0.9166666666666666, 0.9666666666666667, 1.0, 1.0, 0.9833333333333333, 0.95, 1.0, 1.0, 1.0, 0.9166666666666666], [0.975, 0.85, 0.75, 0.9, 0.9166666666666666, 0.85, 0.9666666666666667, 0.8333333333333334, 0.9833333333333333, 0.9666666666666667, 0.9833333333333333, 0.75, 1.0, 1.0, 0.9666666666666667, 0.9833333333333333], [0.85, 0.875, 1.0, 0.975, 0.9, 0.95, 0.9333333333333333, 0.8833333333333333, 1.0, 1.0, 0.9833333333333333, 0.95, 1.0, 1.0, 0.9666666666666667, 0.9], [0.45, 0.75, 0.9, 0.85, 0.9166666666666666, 0.9666666666666667, 0.8, 0.9166666666666666, 1.0, 1.0, 0.9166666666666666, 1.0, 1.0, 1.0, 0.9, 0.9666666666666667], [0.95, 0.725, 0.925, 0.825, 0.85, 0.8833333333333333, 1.0, 0.9333333333333333, 1.0, 1.0, 0.9333333333333333, 0.9666666666666667, 1.0, 1.0, 1.0, 1.0], [0.825, 0.925, 0.825, 0.775, 0.85, 0.95, 0.9333333333333333, 0.95, 1.0, 0.9833333333333333, 0.85, 1.0, 1.0, 1.0, 1.0, 0.9833333333333333], [1.0, 0.825, 0.925, 0.85, 0.8333333333333334, 0.9666666666666667, 0.8333333333333334, 0.9, 1.0, 1.0, 0.9666666666666667, 0.9666666666666667, 1.0, 1.0, 1.0, 0.9166666666666666], [0.825, 0.975, 0.925, 0.725, 0.85, 0.9166666666666666, 0.9333333333333333, 0.95, 1.0, 1.0, 0.5833333333333334, 0.9, 1.0, 0.9833333333333333, 0.9333333333333333, 0.9666666666666667], [0.85, 0.875, 0.9, 0.875, 0.9833333333333333, 0.8666666666666667, 0.9333333333333333, 0.7833333333333333, 1.0, 0.95, 1.0, 0.95, 1.0, 1.0, 0.9333333333333333, 1.0], [0.9, 0.875, 0.925, 0.75, 0.6666666666666666, 0.95, 0.95, 0.9166666666666666, 1.0, 0.9833333333333333, 0.8, 0.9666666666666667, 1.0, 1.0, 0.9833333333333333, 0.95], [0.925, 0.7, 0.85, 0.85, 0.85, 0.8166666666666667, 0.85, 0.9833333333333333, 1.0, 0.9833333333333333, 0.9666666666666667, 0.9, 1.0, 1.0, 0.9833333333333333, 0.9], [0.875, 0.9, 0.775, 0.875, 0.8666666666666667, 1.0, 0.8833333333333333, 0.95, 1.0, 0.9833333333333333, 0.7, 0.9166666666666666, 0.9833333333333333, 1.0, 0.95, 0.9833333333333333], [0.9, 0.875, 0.75, 0.95, 0.8666666666666667, 0.9333333333333333, 0.9333333333333333, 0.9, 0.9833333333333333, 0.9833333333333333, 0.8333333333333334, 0.95, 1.0, 1.0, 0.9833333333333333, 0.9666666666666667], [0.825, 0.975, 0.9, 0.85, 0.8833333333333333, 0.8833333333333333, 0.95, 0.7833333333333333, 1.0, 0.9666666666666667, 1.0, 0.85, 1.0, 1.0, 0.9833333333333333, 0.9666666666666667], [0.825, 0.875, 0.825, 0.875, 0.7166666666666667, 0.85, 0.8833333333333333, 0.7333333333333333, 1.0, 1.0, 0.6, 0.9, 1.0, 1.0, 1.0, 0.9], [0.95, 0.9, 0.925, 0.875, 0.9333333333333333, 1.0, 0.8833333333333333, 0.85, 1.0, 1.0, 0.9166666666666666, 0.9, 1.0, 0.9833333333333333, 1.0, 0.9333333333333333], [0.8, 0.925, 0.825, 0.95, 0.9166666666666666, 0.7666666666666667, 0.9, 0.9, 1.0, 0.9666666666666667, 0.9333333333333333, 0.9833333333333333, 1.0, 1.0, 1.0, 0.9333333333333333], [0.85, 0.8, 0.825, 0.825, 0.9666666666666667, 0.7666666666666667, 0.9666666666666667, 0.9833333333333333, 1.0, 0.9833333333333333, 0.9333333333333333, 0.9666666666666667, 1.0, 1.0, 1.0, 0.9833333333333333], [0.775, 0.925, 0.85, 0.925, 0.9333333333333333, 0.9166666666666666, 0.8666666666666667, 0.95, 1.0, 0.9833333333333333, 1.0, 0.85, 1.0, 1.0, 0.95, 0.9666666666666667], [0.75, 0.625, 0.925, 0.85, 0.8166666666666667, 0.95, 0.9333333333333333, 0.9166666666666666, 1.0, 0.95, 0.9, 0.9333333333333333, 1.0, 1.0, 0.9833333333333333, 0.95], [0.55, 0.825, 0.55, 0.925, 0.8666666666666667, 0.8666666666666667, 0.95, 0.9, 1.0, 0.9666666666666667, 0.9833333333333333, 0.9833333333333333, 1.0, 1.0, 0.9833333333333333, 0.9833333333333333], [0.95, 0.9, 0.975, 0.825, 0.65, 0.9, 0.9166666666666666, 0.9166666666666666, 0.9833333333333333, 0.95, 0.9166666666666666, 1.0, 1.0, 0.9666666666666667, 0.9166666666666666, 0.95], [0.75, 0.925, 0.8, 0.85, 0.9166666666666666, 0.8, 0.9333333333333333, 0.95, 1.0, 0.95, 0.9833333333333333, 0.9333333333333333, 0.9833333333333333, 1.0, 1.0, 0.95], [0.85, 0.875, 0.75, 0.85, 0.8166666666666667, 0.9, 1.0, 0.9333333333333333, 1.0, 0.9833333333333333, 0.95, 0.95, 1.0, 1.0, 0.95, 0.95], [0.85, 0.9, 0.675, 0.975, 0.9333333333333333, 0.8666666666666667, 0.9, 0.9166666666666666, 0.9833333333333333, 1.0, 1.0, 0.9333333333333333, 1.0, 0.9833333333333333, 0.9, 0.9666666666666667], [0.725, 0.95, 0.65, 0.9, 0.8833333333333333, 0.9333333333333333, 0.9333333333333333, 0.8666666666666667, 1.0, 0.9833333333333333, 0.9666666666666667, 0.8666666666666667, 0.9833333333333333, 1.0, 0.9666666666666667, 0.9333333333333333], [0.925, 0.85, 0.825, 1.0, 0.8, 0.8, 0.6333333333333333, 0.8833333333333333, 0.9833333333333333, 1.0, 1.0, 0.9166666666666666, 0.9833333333333333, 1.0, 0.9666666666666667, 1.0], [0.875, 0.825, 0.975, 0.8, 0.9, 0.95, 0.9, 0.9333333333333333, 1.0, 0.9, 0.95, 0.9166666666666666, 1.0, 1.0, 1.0, 0.9333333333333333], [0.9, 0.7, 0.9, 0.85, 0.8666666666666667, 0.9666666666666667, 0.85, 0.9666666666666667, 0.9833333333333333, 0.95, 0.8666666666666667, 0.9166666666666666, 1.0, 1.0, 0.9833333333333333, 0.9333333333333333], [0.75, 0.875, 0.775, 0.925, 0.9166666666666666, 0.9, 0.8166666666666667, 0.9333333333333333, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9833333333333333, 1.0], [0.9, 0.85, 0.675, 0.925, 0.4666666666666667, 0.9166666666666666, 0.8833333333333333, 0.7666666666666667, 1.0, 0.8666666666666667, 1.0, 0.9666666666666667, 0.95, 1.0, 0.9666666666666667, 0.9666666666666667], [0.7, 0.85, 0.65, 0.85, 0.85, 0.8666666666666667, 0.9166666666666666, 0.8666666666666667, 1.0, 0.9166666666666666, 0.9, 0.9, 1.0, 0.9833333333333333, 0.9166666666666666, 0.9166666666666666], [0.925, 0.775, 0.9, 0.8, 0.7166666666666667, 0.95, 0.95, 0.9666666666666667, 0.9833333333333333, 1.0, 0.9, 0.8833333333333333, 1.0, 0.95, 1.0, 0.95], [0.725, 0.8, 0.925, 0.8, 0.8, 0.75, 0.9333333333333333, 0.8833333333333333, 1.0, 0.9833333333333333, 0.9666666666666667, 0.95, 1.0, 1.0, 1.0, 1.0], [0.825, 0.925, 0.55, 0.9, 0.7, 0.9666666666666667, 0.7833333333333333, 0.9666666666666667, 0.9833333333333333, 1.0, 1.0, 0.9333333333333333, 1.0, 0.9833333333333333, 0.9166666666666666, 0.95], [0.775, 0.875, 1.0, 0.85, 0.8, 0.9666666666666667, 0.9166666666666666, 0.9666666666666667, 0.9833333333333333, 1.0, 0.5, 0.9666666666666667, 1.0, 0.9833333333333333, 0.95, 0.9666666666666667], [0.85, 0.9, 1.0, 0.95, 0.9166666666666666, 0.8, 0.9166666666666666, 0.85, 1.0, 0.9166666666666666, 0.9833333333333333, 0.8833333333333333, 1.0, 1.0, 0.9666666666666667, 0.95], [0.9, 0.925, 0.825, 0.825, 0.5833333333333334, 0.9, 0.85, 0.8666666666666667, 1.0, 0.9833333333333333, 0.9333333333333333, 0.8833333333333333, 1.0, 0.9833333333333333, 0.9833333333333333, 0.9666666666666667], [0.8, 0.875, 0.675, 0.925, 0.8666666666666667, 0.9333333333333333, 0.9833333333333333, 0.9333333333333333, 0.9833333333333333, 0.8833333333333333, 0.9666666666666667, 0.95, 1.0, 1.0, 0.95, 0.9833333333333333], [0.875, 0.925, 0.725, 0.575, 0.9333333333333333, 0.9166666666666666, 0.9666666666666667, 0.6833333333333333, 1.0, 0.9833333333333333, 0.9666666666666667, 0.9666666666666667, 1.0, 1.0, 0.95, 0.9666666666666667], [0.875, 0.9, 0.8, 0.9, 0.9, 0.9, 0.85, 0.9, 1.0, 0.9833333333333333, 0.9166666666666666, 0.8666666666666667, 1.0, 1.0, 0.9833333333333333, 0.9833333333333333], [0.775, 0.95, 0.875, 0.925, 0.8166666666666667, 0.8333333333333334, 0.8666666666666667, 0.9, 0.9833333333333333, 1.0, 0.9, 0.9666666666666667, 1.0, 0.9833333333333333, 0.9666666666666667, 0.85], [0.85, 0.975, 0.9, 0.85, 0.7833333333333333, 0.9, 0.8, 0.9333333333333333, 0.9833333333333333, 0.95, 0.8833333333333333, 0.8666666666666667, 1.0, 1.0, 0.95, 0.9], [0.9, 0.7, 0.95, 0.925, 0.8666666666666667, 0.9666666666666667, 0.9333333333333333, 0.8833333333333333, 1.0, 0.9666666666666667, 0.9666666666666667, 1.0, 1.0, 1.0, 0.9333333333333333, 0.9666666666666667], [0.8, 0.95, 0.95, 0.875, 0.7833333333333333, 0.9, 0.85, 0.9, 1.0, 0.9666666666666667, 0.9833333333333333, 0.95, 1.0, 1.0, 1.0, 1.0]]) \n",
    "results = np.concatenate((results_1,results_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt =np.array([x[:8] for x in results]) \n",
    "exp_patterns =np.array([x[8:12] for x in results])\n",
    "exp_multiple_patterns =np.array([x[12:16] for x in results])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of best accuracy scores for every scenario:\n",
      "Scenario no: 1 \n",
      " TImes that this scenario had the best accuracy score: 6\n",
      "Scenario no: 2 \n",
      " TImes that this scenario had the best accuracy score: 15\n",
      "Scenario no: 3 \n",
      " TImes that this scenario had the best accuracy score: 11\n",
      "Scenario no: 4 \n",
      " TImes that this scenario had the best accuracy score: 14\n",
      "Scenario no: 5 \n",
      " TImes that this scenario had the best accuracy score: 7\n",
      "Scenario no: 6 \n",
      " TImes that this scenario had the best accuracy score: 15\n",
      "Scenario no: 7 \n",
      " TImes that this scenario had the best accuracy score: 16\n",
      "Scenario no: 8 \n",
      " TImes that this scenario had the best accuracy score: 16\n",
      "Scenario with the best accuracy scores is scenario no  6\n",
      "\n",
      "Mean value of best acc scores is: 0.9631666666666666\n",
      "\n",
      "Bad results: 132\n",
      "Not so bad results: 276\n",
      "Excellent results: 379\n",
      "Perfect results: 13\n",
      "% bad:  16.5\n",
      "% not so bad:  34.5\n",
      "% excellent:  47.375\n",
      "% perfect:  1.625\n",
      "Mean accuracy score 0.867625\n"
     ]
    }
   ],
   "source": [
    "opt_r = []\n",
    "opt_s = []\n",
    "\n",
    "for x in opt:\n",
    "    opt_r.append(np.amax(x))\n",
    "    opt_s.append(np.argmax(x))\n",
    "counts = np.bincount(opt_s,minlength=8)\n",
    "print(\"Number of best accuracy scores for every scenario:\")\n",
    "for idx,x in enumerate(counts):\n",
    "    print (\"Scenario no:\",idx+1,\"\\n TImes that this scenario had the best accuracy score:\" ,x)\n",
    "print(\"Scenario with the best accuracy scores is scenario no \",np.argmax(counts))   \n",
    "print(\"\\nMean value of best acc scores is:\",np.mean(opt_r))\n",
    "worst = []\n",
    "not_so_bad = []\n",
    "excellent = []\n",
    "perfect = []\n",
    "for x in opt :\n",
    "    for a in x:\n",
    "        if a<0.80:\n",
    "            worst.append(a)\n",
    "        elif 0.80<=a<0.90:\n",
    "            not_so_bad.append(a)\n",
    "        elif 0.90<=a<1:\n",
    "            excellent.append(a)\n",
    "        elif a==1:\n",
    "            perfect.append(a)\n",
    "print(\"\\nBad results:\",len(worst))\n",
    "print(\"Not so bad results:\",len(not_so_bad))\n",
    "print(\"Excellent results:\",len(excellent))\n",
    "print(\"Perfect results:\",len(perfect))\n",
    "print(\"% bad: \",(len(worst)*100)/(100*8))\n",
    "print(\"% not so bad: \",(len(not_so_bad)*100)/(100*8))\n",
    "print(\"% excellent: \",(len(excellent)*100)/(100*8))\n",
    "print(\"% perfect: \",(len(perfect)*100)/(100*8))\n",
    "print(\"Mean accuracy score\",np.mean(opt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of best accuracy scores for every scenario:\n",
      "Scenario no: 1 \n",
      " Times that this scenario had the best accuracy score: 84\n",
      "Scenario no: 2 \n",
      " Times that this scenario had the best accuracy score: 15\n",
      "Scenario no: 3 \n",
      " Times that this scenario had the best accuracy score: 0\n",
      "Scenario no: 4 \n",
      " Times that this scenario had the best accuracy score: 1\n",
      "\n",
      "Scenario with the best accuracy scores is scenario no 0\n",
      "\n",
      "occurrences for max acc results[1>x,1=x]: [14 86]\n",
      "\n",
      "Bad results: 15\n",
      "Not so bad results: 34\n",
      "Excellent results: 220\n",
      "Perfect results: 131\n",
      "% bad:  1.875\n",
      "% not so bad:  4.25\n",
      "% excellent:  27.5\n",
      "% perfect:  16.375\n",
      "Mean accuracy score 0.9515833333333333\n"
     ]
    }
   ],
   "source": [
    "exp_patterns_r = []\n",
    "exp_patterns_s = []\n",
    "\n",
    "for x in exp_patterns:\n",
    "    exp_patterns_r.append(np.amax(x))\n",
    "    exp_patterns_s.append(np.argmax(x))\n",
    "counts = np.bincount(exp_patterns_s,minlength=4)\n",
    "print(\"Number of best accuracy scores for every scenario:\")\n",
    "for idx,x in enumerate(counts):\n",
    "    print (\"Scenario no:\",idx+1,\"\\n Times that this scenario had the best accuracy score:\" ,x)\n",
    "print(\"\\nScenario with the best accuracy scores is scenario no\",np.argmax(counts)) \n",
    "counts_max=np.bincount(exp_patterns_r)\n",
    "print(\"\\noccurrences for max acc results[1>x,1=x]:\",counts_max)\n",
    "worst = []\n",
    "not_so_bad = []\n",
    "excellent = []\n",
    "perfect = []\n",
    "for x in exp_patterns :\n",
    "    for a in x:\n",
    "        if a<0.80:\n",
    "            worst.append(a)\n",
    "        elif 0.80<=a<0.90:\n",
    "            not_so_bad.append(a)\n",
    "        elif 0.90<=a<1:\n",
    "            excellent.append(a)\n",
    "        elif a==1:\n",
    "            perfect.append(a)\n",
    "print(\"\\nBad results:\",len(worst))\n",
    "print(\"Not so bad results:\",len(not_so_bad))\n",
    "print(\"Excellent results:\",len(excellent))\n",
    "print(\"Perfect results:\",len(perfect))\n",
    "print(\"% bad: \",(len(worst)*100)/(100*8))\n",
    "print(\"% not so bad: \",(len(not_so_bad)*100)/(100*8))\n",
    "print(\"% excellent: \",(len(excellent)*100)/(100*8))\n",
    "print(\"% perfect: \",(len(perfect)*100)/(100*8))\n",
    "print(\"Mean accuracy score\",np.mean(exp_patterns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of best accuracy scores for every scenario:\n",
      "Scenario no: 1 \n",
      " Times that this scenario had the best accuracy score: 87\n",
      "Scenario no: 2 \n",
      " Times that this scenario had the best accuracy score: 12\n",
      "Scenario no: 3 \n",
      " Times that this scenario had the best accuracy score: 0\n",
      "Scenario no: 4 \n",
      " Times that this scenario had the best accuracy score: 1\n",
      "\n",
      "Scenario with the best accuracy scores is scenario no 0\n",
      "\n",
      "occurrences for max acc results[1>x,1=x]: [14 86]\n",
      "\n",
      "Bad results: 1\n",
      "Not so bad results: 6\n",
      "Excellent results: 197\n",
      "Perfect results: 196\n",
      "% bad:  0.125\n",
      "% not so bad:  0.75\n",
      "% excellent:  24.625\n",
      "% perfect:  24.5\n",
      "Mean accuracy score 0.9515833333333333\n"
     ]
    }
   ],
   "source": [
    "exp_multiple_pattern_r = []\n",
    "exp_multiple_pattern_s = []\n",
    "\n",
    "for x in exp_multiple_patterns:\n",
    "    exp_multiple_pattern_r.append(np.amax(x))\n",
    "    exp_multiple_pattern_s.append(np.argmax(x))\n",
    "counts = np.bincount(exp_multiple_pattern_s,minlength=4)\n",
    "print(\"Number of best accuracy scores for every scenario:\")\n",
    "for idx,x in enumerate(counts):\n",
    "    print (\"Scenario no:\",idx+1,\"\\n Times that this scenario had the best accuracy score:\" ,x)\n",
    "print(\"\\nScenario with the best accuracy scores is scenario no\",np.argmax(counts)) \n",
    "counts_max=np.bincount(exp_patterns_r)\n",
    "print(\"\\noccurrences for max acc results[1>x,1=x]:\",counts_max)\n",
    "worst = []\n",
    "not_so_bad = []\n",
    "excellent = []\n",
    "perfect = []\n",
    "for x in exp_multiple_patterns :\n",
    "    for a in x:\n",
    "        if a<0.80:\n",
    "            worst.append(a)\n",
    "        elif 0.80<=a<0.90:\n",
    "            not_so_bad.append(a)\n",
    "        elif 0.90<=a<1:\n",
    "            excellent.append(a)\n",
    "        elif a==1:\n",
    "            perfect.append(a)\n",
    "print(\"\\nBad results:\",len(worst))\n",
    "print(\"Not so bad results:\",len(not_so_bad))\n",
    "print(\"Excellent results:\",len(excellent))\n",
    "print(\"Perfect results:\",len(perfect))\n",
    "print(\"% bad: \",(len(worst)*100)/(100*8))\n",
    "print(\"% not so bad: \",(len(not_so_bad)*100)/(100*8))\n",
    "print(\"% excellent: \",(len(excellent)*100)/(100*8))\n",
    "print(\"% perfect: \",(len(perfect)*100)/(100*8))\n",
    "print(\"Mean accuracy score\",np.mean(exp_patterns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
